{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce64c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83ec47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "\n",
    "# import torch.distributed as dist\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.multiprocessing as mp\n",
    "# from ddputils import is_main_process, save_on_master, setup_for_distributed\n",
    "\n",
    "\n",
    "# from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import itertools\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e43602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensors import (\n",
    "    trunc_normal_,\n",
    "    repeat_interleave_batch,\n",
    "    apply_masks\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d793977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "#                                     int(self.patch_embed.num_patches**.5),\n",
    "#                                     cls_token=False)\n",
    "    \n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_h = np.arange(grid_size, dtype=float)\n",
    "    grid_w = np.arange(grid_size, dtype=float)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid length\n",
    "    return:\n",
    "    pos_embed: [grid_size, embed_dim] or [1+grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid = np.arange(grid_size, dtype=float)\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=float)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega   # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)   # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)   # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d87056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "     #@@@\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, num_frames=1, \n",
    "                 tubelet_size=1, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "#         num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        num_patches = (\n",
    "            (img_size // patch_size) * (img_size // patch_size) * (num_frames // tubelet_size)\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.tubelet_size = tubelet_size\n",
    "\n",
    "#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_channels=in_chans,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=(self.tubelet_size, patch_size, patch_size),\n",
    "            stride=(self.tubelet_size, patch_size, patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Convolution stems for ViT following ViTC models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, strides, img_size=224, in_chans=3, batch_norm=True):\n",
    "        super().__init__()\n",
    "        # Build the stems\n",
    "        stem = []\n",
    "        channels = [in_chans] + channels\n",
    "        for i in range(len(channels) - 2):\n",
    "            stem += [nn.Conv2d(channels[i], channels[i+1], kernel_size=3,\n",
    "                               stride=strides[i], padding=1, bias=(not batch_norm))]\n",
    "            if batch_norm:\n",
    "                stem += [nn.BatchNorm2d(channels[i+1])]\n",
    "            stem += [nn.ReLU(inplace=True)]\n",
    "        stem += [nn.Conv2d(channels[-2], channels[-1], kernel_size=1, stride=strides[-1])]\n",
    "        self.stem = nn.Sequential(*stem)\n",
    "\n",
    "        # Comptute the number of patches\n",
    "        stride_prod = int(np.prod(strides))\n",
    "        self.num_patches = (img_size[0] // stride_prod)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.stem(x)\n",
    "        return p.flatten(2).transpose(1, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880a3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_frames=1,\n",
    "        tubelet_size=1,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "         #@@@\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            tubelet_size=tubelet_size,\n",
    "            num_frames=num_frames,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        print('self.pos_embed shape:',self.pos_embed.shape)\n",
    "        \n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "                                            int(self.patch_embed.num_patches**.5),\n",
    "                                            cls_token=False)\n",
    "        print('pos_embed shape:',pos_embed.shape)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19589e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vit_predictor(**kwargs):\n",
    "    model = VisionTransformerPredictor(\n",
    "        mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_huge(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "VIT_EMBED_DIMS = {\n",
    "    'vit_tiny': 192,\n",
    "    'vit_small': 384,\n",
    "    'vit_base': 768,\n",
    "    'vit_large': 1024,\n",
    "    'vit_huge': 1280,\n",
    "    'vit_giant': 1408,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37224ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #@@@\n",
    "def init_model(\n",
    "    device,\n",
    "    patch_size=16,\n",
    "    tubelet_size=1,\n",
    "    num_frames=1,\n",
    "    model_name='vit_base',\n",
    "    crop_size=224,\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "#     encoder = vit.__dict__[model_name](\n",
    "    encoder = vit_base(\n",
    "        img_size=[crop_size],\n",
    "        patch_size=patch_size,\n",
    "        num_frames=num_frames,\n",
    "        tubelet_size=tubelet_size) #@@@\n",
    "#     predictor = vit.__dict__['vit_predictor'](\n",
    "    predictor = vit_predictor(\n",
    "        sequence_shape=encoder.sequence_shape,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    logger.info(encoder)\n",
    "    return encoder, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15e2e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device= 'cuda:0'#'cpu'\n",
    "patch_size=16\n",
    "crop_size=224\n",
    "pred_depth=6\n",
    "pred_emb_dim=384\n",
    "model_name='vit_base'#'vit_small'\n",
    "tubelet_size=2 #@@@\n",
    "num_frames=6 #@@@\n",
    "\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name,\n",
    "    tubelet_size=tubelet_size,\n",
    "    num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f160c205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*(224//16)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b983e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "583ebd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt = torch.rand(8,6,3,224,224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd7d6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder(xtt)\n",
    "# predictor(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da08b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "class PositionalEncoding3D(nn.Module):\n",
    "    \"\"\"\n",
    "    From https://github.com/tatp22/multidim-positional-encoding/tree/master\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding3D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 6) * 2)\n",
    "        if channels % 2:\n",
    "            channels += 1\n",
    "        self.channels = channels\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.register_buffer(\"cached_penc\", None)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 5d tensor of size (batch_size, x, y, z, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, z, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 5:\n",
    "            raise RuntimeError(\"The input tensor has to be 5d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, y, z, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y, device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_z = torch.arange(z, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y).unsqueeze(1)\n",
    "        emb_z = get_emb(sin_inp_z)\n",
    "        emb = torch.zeros((x, y, z, self.channels * 3), device=tensor.device).type(\n",
    "            tensor.type()\n",
    "        )\n",
    "        emb[:, :, :, : self.channels] = emb_x\n",
    "        emb[:, :, :, self.channels : 2 * self.channels] = emb_y\n",
    "        emb[:, :, :, 2 * self.channels :] = emb_z\n",
    "\n",
    "        self.cached_penc = emb[None, :, :, :, :orig_ch].repeat(batch_size, 1, 1, 1, 1)\n",
    "        return self.cached_penc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f710a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 14, 14, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_enc_3d_model = PositionalEncoding3D(768)\n",
    "\n",
    "x = torch.rand(8, 6//2, 224//16, 224//16, 768)\n",
    "ytt = p_enc_3d_model(x) # penc_no_sum.shape == (1, 6, 10)\n",
    "ytt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bcec6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(ytt[0,...]==ytt[1,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dab8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "667bac18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd59d62b130>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ytt[0,1,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "899c78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pairwise_distance as pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b4e3879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6712)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = ytt[0, 0, 1,0,:]\n",
    "x2 = ytt[0, 0, 2,0,:]\n",
    "pdist(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "219167cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9991)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = ytt[0, 0, 0,0,:]\n",
    "x2 = ytt[0, 0, 2,0,:]\n",
    "pdist(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25c5bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x1==x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d30376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_frames=1,\n",
    "        tubelet_size=1,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=12,\n",
    "        predictor_depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # --\n",
    "         #@@@\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            tubelet_size=tubelet_size,\n",
    "            num_frames=num_frames,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        # --\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)\n",
    "        #@@@\n",
    "        sequence_shape = (num_frames//tubelet_size, img_size[0]//patch_size,\n",
    "                       img_size[0]//patch_size)\n",
    "        self.sequence_shape = sequence_shape\n",
    "        p_enc_3d_model = PositionalEncoding3D(embed_dim)\n",
    "        tmp_input = torch.rand(1, *sequence_shape, embed_dim)\n",
    "        pos_embed = p_enc_3d_model(tmp_input).reshape(1,-1,embed_dim)\n",
    "#         pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1],\n",
    "#                                             int(self.patch_embed.num_patches**.5),\n",
    "#                                             cls_token=False)\n",
    "#         self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        self.pos_embed.data.copy_(pos_embed.float())\n",
    "        # --\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks=None):\n",
    "        if masks is not None:\n",
    "            if not isinstance(masks, list):\n",
    "                masks = [masks]\n",
    "\n",
    "        # -- patchify x\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # -- add positional embedding to x\n",
    "        pos_embed = self.interpolate_pos_encoding(x, self.pos_embed)\n",
    "        x = x + pos_embed\n",
    "\n",
    "        # -- mask x\n",
    "        if masks is not None:\n",
    "            x = apply_masks(x, masks)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, pos_embed):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = pos_embed.shape[1] - 1\n",
    "        if npatch == N:\n",
    "            return pos_embed\n",
    "        class_emb = pos_embed[:, 0]\n",
    "        pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        pos_embed = nn.functional.interpolate(\n",
    "            pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=math.sqrt(npatch / N),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_emb.unsqueeze(0), pos_embed), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f307747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerPredictor(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_shape,\n",
    "        embed_dim=768,\n",
    "        predictor_embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = np.prod(sequence_shape)\n",
    "        self.predictor_embed = nn.Linear(embed_dim, predictor_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # --\n",
    "        self.predictor_pos_embed = nn.Parameter(torch.zeros(1, num_patches, predictor_embed_dim),\n",
    "                                                requires_grad=False)\n",
    "        \n",
    "        p_enc_3d_model = PositionalEncoding3D(predictor_embed_dim)\n",
    "        tmp_input = torch.rand(1, *sequence_shape, predictor_embed_dim)\n",
    "        predictor_pos_embed = p_enc_3d_model(tmp_input).reshape(\n",
    "            1,-1,predictor_embed_dim)\n",
    "        self.predictor_pos_embed.data.copy_(predictor_pos_embed.float())\n",
    "\n",
    "#         predictor_pos_embed = get_2d_sincos_pos_embed(self.predictor_pos_embed.shape[-1],\n",
    "#                                                       int(num_patches**.5),\n",
    "#                                                       cls_token=False)\n",
    "#         self.predictor_pos_embed.data.copy_(torch.from_numpy(predictor_pos_embed).float().unsqueeze(0))\n",
    "        # --\n",
    "        self.predictor_blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=predictor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.predictor_norm = norm_layer(predictor_embed_dim)\n",
    "        self.predictor_proj = nn.Linear(predictor_embed_dim, embed_dim, bias=True)\n",
    "        # ------\n",
    "        self.init_std = init_std\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.predictor_blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, masks_x, masks):\n",
    "        assert (masks is not None) and (masks_x is not None), 'Cannot run predictor without mask indices'\n",
    "\n",
    "        if not isinstance(masks_x, list):\n",
    "            masks_x = [masks_x]\n",
    "\n",
    "        if not isinstance(masks, list):\n",
    "            masks = [masks]\n",
    "\n",
    "        # -- Batch Size\n",
    "        B = len(x) // len(masks_x)\n",
    "\n",
    "        # -- map from encoder-dim to pedictor-dim\n",
    "        x = self.predictor_embed(x)\n",
    "\n",
    "        # -- add positional embedding to x tokens\n",
    "        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        x += apply_masks(x_pos_embed, masks_x)\n",
    "\n",
    "        _, N_ctxt, D = x.shape\n",
    "\n",
    "        # -- concat mask tokens to x\n",
    "        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)\n",
    "        pos_embs = apply_masks(pos_embs, masks)\n",
    "        pos_embs = repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))\n",
    "        # --\n",
    "        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)\n",
    "        # --\n",
    "        pred_tokens += pos_embs\n",
    "        x = x.repeat(len(masks), 1, 1)\n",
    "        x = torch.cat([x, pred_tokens], dim=1)\n",
    "\n",
    "        # -- fwd prop\n",
    "        for blk in self.predictor_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.predictor_norm(x)\n",
    "\n",
    "        # -- return preds for mask tokens\n",
    "        x = x[:, N_ctxt:]\n",
    "        x = self.predictor_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac35e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
