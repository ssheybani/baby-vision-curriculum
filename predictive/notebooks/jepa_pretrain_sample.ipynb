{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d202f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "\n",
    "# import torch.distributed as dist\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.multiprocessing as mp\n",
    "# from ddputils import is_main_process, save_on_master, setup_for_distributed\n",
    "\n",
    "\n",
    "# from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import itertools\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2d7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size=224\n",
    "crop_scale=(0.3, 1.0)#1 #0.3\n",
    "use_gaussian_blur=False\n",
    "use_horizontal_flip=False\n",
    "use_color_distortion=False\n",
    "color_jitter=0\n",
    "patch_size=16\n",
    "pred_mask_scale=(0.15, 0.2)\n",
    "enc_mask_scale=(0.85,1)\n",
    "aspect_ratio=(0.75,1.5)\n",
    "num_enc_masks=1\n",
    "num_pred_masks=4\n",
    "allow_overlap=False\n",
    "min_keep=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5360549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist\n",
    "    \n",
    "def get_train_val_split(fpathlist, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the list of filepaths into a train list and val list\n",
    "    \"\"\"\n",
    "    n_fr = len(fpathlist)\n",
    "    val_size = int(n_fr*val_ratio)\n",
    "    \n",
    "    split1_idx = int((n_fr-val_size)/2)\n",
    "    split2_idx = int((n_fr+val_size)/2)\n",
    "    train_set =fpathlist[:split1_idx]+fpathlist[split2_idx:]\n",
    "    val_set = fpathlist[split1_idx:split2_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_fpathseqlist(fpathlist, seq_len, ds_rate=1, n_samples=None):\n",
    "    \"\"\"\n",
    "    Returns a list of list that can be passed to ImageSequenceDataset\n",
    "    # n_samples: int\n",
    "    # between 1 and len(fpathlist)\n",
    "    # If None, it's set to len(fpathlist)/seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_len = seq_len*ds_rate\n",
    "    if n_samples is None:\n",
    "        n_samples = int(len(fpathlist)/seq_len)\n",
    "        sample_stride = sample_len\n",
    "    else:\n",
    "        assert type(n_samples)==int\n",
    "        assert len(fpathlist)>n_samples\n",
    "        sample_stride = int(len(fpathlist)/n_samples)\n",
    "        # for adult group, sample_stride ~=10. i.e. each frame contributes to more than 1 sample sequence, \n",
    "        # but doesn't appear in the same index of the sequence.\n",
    "\n",
    "    fpathseqlist = [fpathlist[i:i+sample_len:ds_rate] \n",
    "                    for i in range(0, n_samples*sample_stride, sample_stride)]\n",
    "    return fpathseqlist\n",
    "\n",
    "\n",
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "    \n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)\n",
    "\n",
    "def get_fold(gx_fpathlist, fold, max_folds, args):\n",
    "#     fold_size = int(len(gx_fpathlist)/max_folds)\n",
    "    segment_size = int(30*60*30/args.ds_rate)\n",
    "    \n",
    "    fold_segments = []\n",
    "\n",
    "    for i_st in range(0, len(gx_fpathlist), segment_size):\n",
    "        if (i_st // segment_size) % max_folds == fold:\n",
    "            fold_segments.append(gx_fpathlist[i_st:i_st + segment_size])\n",
    "            \n",
    "    fold_segments = list(itertools.chain.from_iterable(fold_segments))\n",
    "    return fold_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0b54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def make_transforms(\n",
    "    crop_size=224,\n",
    "    crop_scale=(0.3, 1.0),\n",
    "    normalization=((0.5, 0.5, 0.5),\n",
    "                   (0.25, 0.25, 0.25))\n",
    "):\n",
    "\n",
    "    transform_list = []\n",
    "    transform_list += [transforms.RandomResizedCrop(crop_size, scale=crop_scale)]\n",
    "#     transform_list += [transforms.ConvertImageDtype(torch.float32)] \n",
    "    transform_list += [transforms.ToTensor()]\n",
    "    transform_list += [transforms.Normalize(normalization[0], normalization[1])]\n",
    "\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "\n",
    "# transform = make_transforms(\n",
    "#     crop_size=crop_size,\n",
    "#     crop_scale=crop_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c2d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageSequenceDataset(Dataset):\n",
    "import PIL\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform, shuffle=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "        fp = self.image_paths[idx][0]\n",
    "        images = self.transform(PIL.Image.open(fp))\n",
    "#         images = self.transform(torchvision.io.read_image(fp))\n",
    "#         images = torch.cat([\n",
    "#             self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\n",
    "#                      for fp in self.image_paths[idx]]) #with tochvision transform\n",
    "        \n",
    "#         if self.shuffle:\n",
    "#             size = images.size(0)\n",
    "#             perm = torch.randperm(size)\n",
    "#             images = images[perm]\n",
    "            \n",
    "        return images\n",
    "\n",
    "def make_dataset(subj_dirs, image_size, args):\n",
    "    seq_len = args.num_frames #kwargs['seq_len']\n",
    "#     n_groupframes=kwargs['n_groupframes']#1450000\n",
    "    ds_rate = args.ds_rate #kwargs['ds_rate']\n",
    "    jpg_root = args.jpg_root #kwargs['jpg_root']\n",
    "#     image_size = kwargs['image_size']\n",
    "    fold = args.fold #kwargs['fold']\n",
    "    condition = args.condition #kwargs['condition']\n",
    "    n_trainsamples = args.n_trainsamples\n",
    "    \n",
    "    crop_size = args.crop_size\n",
    "    crop_scale = args.crop_scale\n",
    "    \n",
    "    transform = make_transforms(\n",
    "        crop_size=crop_size,\n",
    "        crop_scale=crop_scale)\n",
    "    #_get_transform(image_size)\n",
    "    gx_fpathlist = []\n",
    "    for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "        gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)\n",
    "    \n",
    "    # added on May15\n",
    "    max_folds = 3\n",
    "    gx_fpathlist = get_fold(gx_fpathlist, fold, max_folds, args)\n",
    "    print('Num. frames in the fold:',len(gx_fpathlist))\n",
    "\n",
    "    #     if len(gx_fpathlist)>=n_groupframes:\n",
    "#         gx_fpathlist = gx_fpathlist[:n_groupframes]\n",
    "#         # 1450000/16 = 90625 => n_trainsamples=81560, n_valsamples= 9060\n",
    "#         # 1274 iterations of train. 141 iterations of test. \n",
    "#         n_trainsamples = None\n",
    "#         n_valsamples = None\n",
    "#     else:\n",
    "    \n",
    "    # Train-val split\n",
    "    gx_train_fp, gx_val_fp = get_train_val_split(gx_fpathlist, val_ratio=0.1)\n",
    "\n",
    "    if condition=='longshuffle':\n",
    "        random.shuffle(gx_train_fp)\n",
    "    \n",
    "    n_trainsamples = n_trainsamples #int(0.9*n_groupframes/seq_len) #81k\n",
    "    \n",
    "    n_maxvalsamples = int(len(gx_val_fp)/seq_len)\n",
    "    n_valsamples = min(n_maxvalsamples, 10000)  #means don't do bootstraping for val. Use whatever number 0.1*len(gx_fpathlist) gives.\n",
    "    \n",
    "    gx_train_fpathseqlist = get_fpathseqlist(gx_train_fp, seq_len, ds_rate=1, n_samples=n_trainsamples)\n",
    "    gx_val_fpathseqlist = get_fpathseqlist(gx_val_fp, seq_len, ds_rate=1, n_samples=n_valsamples)\n",
    "    \n",
    "    \n",
    "    if condition=='shuffle':\n",
    "        train_dataset = ImageSequenceDataset(gx_train_fpathseqlist, transform=transform, shuffle=True)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "    \n",
    "    elif condition=='static':\n",
    "        train_dataset = StillVideoDataset(gx_train_fpathseqlist, transform=transform)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "    elif condition=='image':\n",
    "        train_dataset = ImageDataset(gx_train_fpathseqlist, transform=transform)\n",
    "        val_dataset = ImageDataset(gx_val_fpathseqlist, transform=transform)\n",
    "#         StillVideoDataset(gx_val_fpathseqlist, transform=transform)\n",
    "\n",
    "    else:\n",
    "        train_dataset = ImageSequenceDataset(gx_train_fpathseqlist, transform=transform, shuffle=False)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "        \n",
    "    return {'train':train_dataset,\n",
    "           'val': val_dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d4e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b86ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2107a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_root='/N/project/baby_vision_curriculum/homeview_subset_30fps/'\n",
    "data_seed=401\n",
    "\n",
    "saveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/jul17/'\n",
    "tbsaveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/benchmarks/toybox/jul17/'\n",
    "ucsaveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/benchmarks/ucf101/jul17/'\n",
    "\n",
    "n_epoch=5\n",
    "curr='yo'\n",
    "condition='image'\n",
    "other_id=curr+'_'+condition\n",
    "monitor='grad'\n",
    "ds_rate=1\n",
    "batch_size=256#64#16\n",
    "n_trainsamples=12000#162000\n",
    "max_epoch_iters=2000\n",
    "tbbatch_size=64\n",
    "ucbatch_size=64\n",
    "mask_sampler='tube'\n",
    "tubelet_size=1\n",
    "num_frames=1\n",
    "\n",
    "train_group='g2'#'g0'\n",
    "fold=data_seed % 3\n",
    "# mask_ratio=0.9\n",
    "#adamw: lr=1.5e-4, wd=0.05, momentum=0.9 (doesn't get used)\n",
    "#adam: lr=0.001. wd=1e-4, momentum=0.9 (doesn't get used)\n",
    "#sgd: 0.1, 0, 0.9\n",
    "optim='sgd'\n",
    "lr=0.1\n",
    "wd=0\n",
    "architecture='small'#'base'\n",
    "momentum=0.9\n",
    "\n",
    "\n",
    "\n",
    "savedir=saveroot+'s1/' #\"${saveroot}s1/\"\n",
    "# Initialization\n",
    "init_checkpoint_path='na'\n",
    "\n",
    "# other_seed=data_seed#$data_seed\n",
    "script='pretrain_vjepa_v0.py'\n",
    "\n",
    "args = Args(jpg_root=jpg_root,\n",
    "            train_group=train_group,\n",
    "            savedir=savedir,\n",
    "            architecture=architecture,\n",
    "            init_checkpoint_path=init_checkpoint_path,\n",
    "            seed=data_seed,\n",
    "            other_id=other_id,\n",
    "            batch_size=batch_size,\n",
    "           num_frames=num_frames,\n",
    "           ds_rate=ds_rate,\n",
    "           fold=fold,\n",
    "           condition=condition,\n",
    "           n_trainsamples=n_trainsamples,\n",
    "           crop_size=crop_size,\n",
    "           crop_scale=crop_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71f055f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['043MP', '044ET', '046TE', '047MS', '048KG', '049JC', '050AB', '050AK', '051DW']\n"
     ]
    }
   ],
   "source": [
    "g0='008MS+009SS+010BF+011EA+012TT+013LS+014SN+015JM+016TF+017EW'\n",
    "g1='026AR+027SS+028CK+028MR+029TT+030FD+031HW+032SR+033SE+034JC'\n",
    "g2='043MP+044ET+046TE+047MS+048KG+049JC+050AB+050AK+051DW'\n",
    "g3='BR+CW+EA+ED+JB+KI+LS+SB+TR'\n",
    "# Total number of frames in each age group: g0=1.68m, g1=1.77m, g2=1.45m\n",
    "\n",
    "g0 = g0.split('+')\n",
    "g1 = g1.split('+')\n",
    "g2 = g2.split('+')\n",
    "g3 = g3.split('+')\n",
    "\n",
    "gRand=[]\n",
    "for gx in [g0,g1,g2,g3]:\n",
    "    gRand.extend(random.sample(gx, 3))\n",
    "random.shuffle(gRand)\n",
    "\n",
    "group_dict = {\"g0\":g0, \"g1\":g1, \"g2\":g2, \"g3\":g3, 'gr':gRand}\n",
    "group = group_dict.get(args.train_group)\n",
    "print(group)                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5bbb8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:22<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. frames in the fold: 1404192\n"
     ]
    }
   ],
   "source": [
    "image_size=224\n",
    "datasets = make_dataset(group, image_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e5116161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from multiprocessing import Value\n",
    "from logging import getLogger\n",
    "\n",
    "_GLOBAL_SEED = 0\n",
    "logger = getLogger()\n",
    "\n",
    "class MBMaskCollator(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=(224, 224),\n",
    "        patch_size=16,\n",
    "        enc_mask_scale=(0.2, 0.8),\n",
    "        pred_mask_scale=(0.2, 0.8),\n",
    "        aspect_ratio=(0.3, 3.0),\n",
    "        nenc=1,\n",
    "        npred=2,\n",
    "        min_keep=4,\n",
    "        allow_overlap=False\n",
    "    ):\n",
    "        super(MBMaskCollator, self).__init__()\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.patch_size = patch_size\n",
    "        self.height, self.width = input_size[0] // patch_size, input_size[1] // patch_size\n",
    "        self.enc_mask_scale = enc_mask_scale\n",
    "        self.pred_mask_scale = pred_mask_scale\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.nenc = nenc\n",
    "        self.npred = npred\n",
    "        self.min_keep = min_keep  # minimum number of patches to keep\n",
    "        self.allow_overlap = allow_overlap  # whether to allow overlap b/w enc and pred masks\n",
    "        self._itr_counter = Value('i', -1)  # collator is shared across worker processes\n",
    "\n",
    "    def step(self):\n",
    "        i = self._itr_counter\n",
    "        with i.get_lock():\n",
    "            i.value += 1\n",
    "            v = i.value\n",
    "        return v\n",
    "\n",
    "    def _sample_block_size(self, generator, scale, aspect_ratio_scale):\n",
    "        _rand = torch.rand(1, generator=generator).item()\n",
    "        # -- Sample block scale\n",
    "        min_s, max_s = scale\n",
    "        mask_scale = min_s + _rand * (max_s - min_s)\n",
    "        max_keep = int(self.height * self.width * mask_scale)\n",
    "        # -- Sample block aspect-ratio\n",
    "        min_ar, max_ar = aspect_ratio_scale\n",
    "        aspect_ratio = min_ar + _rand * (max_ar - min_ar)\n",
    "        # -- Compute block height and width (given scale and aspect-ratio)\n",
    "        h = int(round(math.sqrt(max_keep * aspect_ratio)))\n",
    "        w = int(round(math.sqrt(max_keep / aspect_ratio)))\n",
    "        while h >= self.height:\n",
    "            h -= 1\n",
    "        while w >= self.width:\n",
    "            w -= 1\n",
    "\n",
    "        return (h, w)\n",
    "\n",
    "    def _sample_block_mask(self, b_size, acceptable_regions=None):\n",
    "        h, w = b_size\n",
    "\n",
    "        def constrain_mask(mask, tries=0):\n",
    "            \"\"\" Helper to restrict given mask to a set of acceptable regions \"\"\"\n",
    "            N = max(int(len(acceptable_regions)-tries), 0)\n",
    "            for k in range(N):\n",
    "                mask *= acceptable_regions[k]\n",
    "        # --\n",
    "        # -- Loop to sample masks until we find a valid one\n",
    "        tries = 0\n",
    "        timeout = og_timeout = 20\n",
    "        valid_mask = False\n",
    "        while not valid_mask:\n",
    "            # -- Sample block top-left corner\n",
    "            top = torch.randint(0, self.height - h, (1,))\n",
    "            left = torch.randint(0, self.width - w, (1,))\n",
    "            mask = torch.zeros((self.height, self.width), dtype=torch.int32)\n",
    "            mask[top:top+h, left:left+w] = 1\n",
    "            # -- Constrain mask to a set of acceptable regions\n",
    "            if acceptable_regions is not None:\n",
    "                constrain_mask(mask, tries)\n",
    "            mask = torch.nonzero(mask.flatten())\n",
    "            # -- If mask too small try again\n",
    "            valid_mask = len(mask) > self.min_keep\n",
    "            if not valid_mask:\n",
    "                timeout -= 1\n",
    "                if timeout == 0:\n",
    "                    tries += 1\n",
    "                    timeout = og_timeout\n",
    "                    logger.warning(f'Mask generator says: \"Valid mask not found, decreasing acceptable-regions [{tries}]\"')\n",
    "        mask = mask.squeeze()\n",
    "        # --\n",
    "        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)\n",
    "        mask_complement[top:top+h, left:left+w] = 0\n",
    "        # --\n",
    "        return mask, mask_complement\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        Create encoder and predictor masks when collating imgs into a batch\n",
    "        # 1. sample enc block (size + location) using seed\n",
    "        # 2. sample pred block (size) using seed\n",
    "        # 3. sample several enc block locations for each image (w/o seed)\n",
    "        # 4. sample several pred block locations for each image (w/o seed)\n",
    "        # 5. return enc mask and pred mask\n",
    "        '''\n",
    "        B = len(batch)\n",
    "\n",
    "        collated_batch = torch.utils.data.default_collate(batch)\n",
    "\n",
    "        seed = self.step()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        p_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.pred_mask_scale,\n",
    "            aspect_ratio_scale=self.aspect_ratio)\n",
    "        e_size = self._sample_block_size(\n",
    "            generator=g,\n",
    "            scale=self.enc_mask_scale,\n",
    "            aspect_ratio_scale=(1., 1.))\n",
    "\n",
    "        collated_masks_pred, collated_masks_enc = [], []\n",
    "        min_keep_pred = self.height * self.width\n",
    "        min_keep_enc = self.height * self.width\n",
    "        for _ in range(B):\n",
    "\n",
    "            masks_p, masks_C = [], []\n",
    "            for _ in range(self.npred):\n",
    "                mask, mask_C = self._sample_block_mask(p_size)\n",
    "                masks_p.append(mask)\n",
    "                masks_C.append(mask_C)\n",
    "                min_keep_pred = min(min_keep_pred, len(mask))\n",
    "            collated_masks_pred.append(masks_p)\n",
    "\n",
    "            acceptable_regions = masks_C\n",
    "            try:\n",
    "                if self.allow_overlap:\n",
    "                    acceptable_regions= None\n",
    "            except Exception as e:\n",
    "                logger.warning(f'Encountered exception in mask-generator {e}')\n",
    "\n",
    "            masks_e = []\n",
    "            for _ in range(self.nenc):\n",
    "                mask, _ = self._sample_block_mask(e_size, acceptable_regions=acceptable_regions)\n",
    "                masks_e.append(mask)\n",
    "                min_keep_enc = min(min_keep_enc, len(mask))\n",
    "            collated_masks_enc.append(masks_e)\n",
    "\n",
    "        collated_masks_pred = [[cm[:min_keep_pred] \n",
    "                                for cm in cm_list] \n",
    "                               for cm_list in collated_masks_pred]\n",
    "        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)\n",
    "        # --\n",
    "        collated_masks_enc = [[cm[:min_keep_enc] \n",
    "                               for cm in cm_list] \n",
    "                              for cm_list in collated_masks_enc]\n",
    "        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)\n",
    "\n",
    "        return collated_batch, collated_masks_enc, collated_masks_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1fe1edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mask_collator = MBMaskCollator(\n",
    "    input_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    pred_mask_scale=pred_mask_scale,\n",
    "    enc_mask_scale=enc_mask_scale,\n",
    "    aspect_ratio=aspect_ratio,\n",
    "    nenc=num_enc_masks,\n",
    "    npred=num_pred_masks,\n",
    "    allow_overlap=allow_overlap,\n",
    "    min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a17e05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory = False\n",
    "# num_workers=\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "        datasets[x], batch_size=batch_size, pin_memory=pin_memory, \n",
    "        shuffle=False, collate_fn=mask_collator,\n",
    "        drop_last=True)\n",
    "                        for x in ['train', 'val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3314b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ceb954a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/46 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "for inputs in tqdm(dataloaders[phase]):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce8ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "udata: torch.Size([3, 224, 224])\n",
      "encoder context: torch.Size([8, 67])\n",
      "prediction target: torch.Size([8, 36])\n",
      "prediction target: torch.Size([8, 36])\n",
      "prediction target: torch.Size([8, 36])\n",
      "prediction target: torch.Size([8, 36])\n"
     ]
    }
   ],
   "source": [
    "for item in inputs[0]:\n",
    "    print('udata:',item.shape)\n",
    "for item in inputs[1]:\n",
    "    print('encoder context:',item.shape)\n",
    "for item in inputs[2]:\n",
    "    print('prediction target:',item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09e64ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import init_model, init_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "226b92e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device= 'cuda:0'#'cpu'\n",
    "model_name='vit_base'#'vit_small'\n",
    "pred_depth=6\n",
    "pred_emb_dim=384\n",
    "    \n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name)\n",
    "target_encoder = deepcopy(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f7d54c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using SGD\n"
     ]
    }
   ],
   "source": [
    "wd=0\n",
    "final_wd=0\n",
    "start_lr=0.1\n",
    "ref_lr=0.1\n",
    "final_lr=0.1\n",
    "ipe=2000 #iterations_per_epoch\n",
    "warmup=0\n",
    "num_epochs=1\n",
    "ipe_scale=1.0\n",
    "use_bfloat16=True\n",
    "\n",
    "optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "    encoder=encoder,\n",
    "    predictor=predictor,\n",
    "    wd=wd,\n",
    "    final_wd=final_wd,\n",
    "    start_lr=start_lr,\n",
    "    ref_lr=lr,\n",
    "    final_lr=final_lr,\n",
    "    iterations_per_epoch=ipe,\n",
    "    warmup=warmup,\n",
    "    num_epochs=num_epochs,\n",
    "    ipe_scale=ipe_scale,\n",
    "    use_bfloat16=use_bfloat16)\n",
    "# encoder = DistributedDataParallel(encoder, static_graph=True)\n",
    "# predictor = DistributedDataParallel(predictor, static_graph=True)\n",
    "# target_encoder = DistributedDataParallel(target_encoder)\n",
    "for p in target_encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d76daa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- momentum schedule\n",
    "ema = (0.996, 1.0)\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
    "                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "86b7b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "start_epoch = 0\n",
    "\n",
    "# --\n",
    "log_timings = True\n",
    "log_freq = 10\n",
    "checkpoint_freq = 50\n",
    "# --\n",
    "\n",
    "load_model = False\n",
    "# -- load training checkpoint\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, _, scaler, _ = load_checkpoint(\n",
    "        device=device,\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "#         scheduler.step()\n",
    "#         wd_scheduler.step()\n",
    "#         next(momentum_scheduler)\n",
    "        mask_collator.step()\n",
    "    \n",
    "def save_checkpoint(epoch):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': None,\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss_meter.avg,\n",
    "        'batch_size': batch_size,\n",
    "        'world_size': world_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "        if (epoch + 1) % checkpoint_freq == 0:\n",
    "            torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1ec477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d57d20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loggingtools import (\n",
    "    CSVLogger,\n",
    "    grad_logger,\n",
    "    AverageMeter,\n",
    "    gpu_timer)\n",
    "\n",
    "import yaml\n",
    "\n",
    "# -- LOGGING\n",
    "folder = '/N/project/baby_vision_curriculum/trained_models/predictive/v0/jul17/'\n",
    "#args['logging']['folder']\n",
    "tag = 'jepa'\n",
    "#args['logging']['write_tag']\n",
    "\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "# try:\n",
    "#     mp.set_start_method('spawn')\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# -- init torch distributed backend\n",
    "# world_size, rank = init_distributed()\n",
    "# logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
    "# if rank > 0:\n",
    "#     logger.setLevel(logging.ERROR)\n",
    "\n",
    "# -- log/checkpointing paths\n",
    "log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
    "save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
    "latest_path = os.path.join(folder, f'{tag}-latest.pth.tar')\n",
    "load_path = None\n",
    "if load_model:\n",
    "    load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
    "\n",
    "# -- make csv_logger\n",
    "csv_logger = CSVLogger(log_file,\n",
    "                       ('%d', 'epoch'),\n",
    "                       ('%d', 'itr'),\n",
    "                       ('%.5f', 'loss'),\n",
    "                       ('%.5f', 'mask-A'),\n",
    "                       ('%.5f', 'mask-B'),\n",
    "                       ('%d', 'time (ms)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "29b06ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tensors import apply_masks, repeat_interleave_batch\n",
    "from distributed import AllReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1a4cb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:[1,     0] loss: 0.415 masks: 49.0 35.0 [mem: 1.80e+04] (391.3 ms)\n",
      "INFO:root:[1,     0] grad_stats: [1.96e-02 1.06e-02] (1.02e-02, 2.24e-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████▎                                 | 10/46 [00:19<01:09,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:[1,    10] loss: 0.240 masks: 50.2 31.8 [mem: 2.04e+04] (392.4 ms)\n",
      "INFO:root:[1,    10] grad_stats: [4.54e-03 1.92e-03] (1.90e-03, 1.13e-02)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████▋                        | 20/46 [00:56<03:01,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:[1,    20] loss: 0.204 masks: 49.1 32.3 [mem: 2.04e+04] (390.0 ms)\n",
      "INFO:root:[1,    20] grad_stats: [3.36e-03 2.05e-03] (1.95e-03, 3.78e-03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████▏                 | 27/46 [03:20<02:21,  7.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [120]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m maskB_meter \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[1;32m     11\u001b[0m time_meter \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr, (udata, masks_enc, masks_pred) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     14\u001b[0m     tqdm(dataloaders[phase])):\n\u001b[1;32m     16\u001b[0m     i_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m itr\u001b[38;5;241m>\u001b[39mi_break:\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Load the sequence of images\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m         images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         images = self.transform(torchvision.io.read_image(fp))\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         images = torch.cat([\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#             self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             perm = torch.randperm(size)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#             images = images[perm]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/PIL/Image.py:3068\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3065\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3068\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3069\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -- TRAINING LOOP\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    logger.info('Epoch %d' % (epoch + 1))\n",
    "\n",
    "    # -- update distributed-data-loader epoch\n",
    "#     unsupervised_sampler.set_epoch(epoch)\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    maskA_meter = AverageMeter()\n",
    "    maskB_meter = AverageMeter()\n",
    "    time_meter = AverageMeter()\n",
    "\n",
    "    for itr, (udata, masks_enc, masks_pred) in enumerate(\n",
    "        tqdm(dataloaders[phase])):\n",
    "        \n",
    "        i_break = 200\n",
    "        if itr>i_break:\n",
    "            break #@@@\n",
    "            \n",
    "        def load_imgs():\n",
    "            # -- unsupervised imgs\n",
    "            imgs = udata.to(device, non_blocking=True)\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            return (imgs, masks_1, masks_2)\n",
    "        imgs, masks_enc, masks_pred = load_imgs()\n",
    "        maskA_meter.update(len(masks_enc[0][0]))\n",
    "        maskB_meter.update(len(masks_pred[0][0]))\n",
    "\n",
    "        def train_step():\n",
    "#             _new_lr = scheduler.step()\n",
    "#             _new_wd = wd_scheduler.step()\n",
    "            # --\n",
    "\n",
    "            def forward_target():\n",
    "                with torch.no_grad():\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
    "                    B = len(h)\n",
    "                    # -- create targets (masked regions of h)\n",
    "                    h = apply_masks(h, masks_pred)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
    "                    return h\n",
    "\n",
    "            def forward_context():\n",
    "                z = encoder(imgs, masks_enc)\n",
    "                z = predictor(z, masks_enc, masks_pred)\n",
    "                return z\n",
    "\n",
    "            def loss_fn(z, h):\n",
    "                loss = F.smooth_l1_loss(z, h)\n",
    "                loss = AllReduce.apply(loss)\n",
    "                return loss\n",
    "\n",
    "            # Step 1. Forward\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, \n",
    "                                         enabled=use_bfloat16):\n",
    "                h = forward_target()\n",
    "                z = forward_context()\n",
    "                loss = loss_fn(z, h)\n",
    "\n",
    "            #  Step 2. Backward & step\n",
    "            if use_bfloat16:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            grad_stats = grad_logger(encoder.named_parameters())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3. momentum update of target encoder\n",
    "            with torch.no_grad():\n",
    "                m = next(momentum_scheduler)\n",
    "                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "\n",
    "            return (float(loss), grad_stats)\n",
    "        loss, grad_stats = train_step()\n",
    "        (loss, grad_stats), etime = gpu_timer(train_step)\n",
    "#         etime=0 #@@@\n",
    "#         (loss, _new_lr, _new_wd, grad_stats), etime = gpu_timer(train_step)\n",
    "        loss_meter.update(loss)\n",
    "        time_meter.update(etime)\n",
    "\n",
    "        # -- Logging\n",
    "        def log_stats():\n",
    "            csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
    "            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
    "                logger.info('[%d, %5d] loss: %.3f '\n",
    "                            'masks: %.1f %.1f '\n",
    "                            '[mem: %.2e] '\n",
    "                            '(%.1f ms)'\n",
    "                            % (epoch + 1, itr,\n",
    "                               loss_meter.avg,\n",
    "                               maskA_meter.avg,\n",
    "                               maskB_meter.avg,\n",
    "                               torch.cuda.max_memory_allocated() / 1024.**2,\n",
    "                               time_meter.avg))\n",
    "\n",
    "                if grad_stats is not None:\n",
    "                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
    "                                % (epoch + 1, itr,\n",
    "                                   grad_stats.first_layer,\n",
    "                                   grad_stats.last_layer,\n",
    "                                   grad_stats.min,\n",
    "                                   grad_stats.max))\n",
    "\n",
    "        log_stats()\n",
    "\n",
    "        assert not np.isnan(loss), 'loss is nan'\n",
    "\n",
    "    # -- Save Checkpoint after every epoch\n",
    "#     logger.info('avg. loss %.3f' % loss_meter.avg)\n",
    "#         save_checkpoint(epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
