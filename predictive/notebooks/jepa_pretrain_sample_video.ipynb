{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d202f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "\n",
    "# import torch.distributed as dist\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# import torch.multiprocessing as mp\n",
    "# from ddputils import is_main_process, save_on_master, setup_for_distributed\n",
    "\n",
    "\n",
    "# from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5360549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist\n",
    "    \n",
    "def get_train_val_split(fpathlist, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the list of filepaths into a train list and val list\n",
    "    \"\"\"\n",
    "    n_fr = len(fpathlist)\n",
    "    val_size = int(n_fr*val_ratio)\n",
    "    \n",
    "    split1_idx = int((n_fr-val_size)/2)\n",
    "    split2_idx = int((n_fr+val_size)/2)\n",
    "    train_set =fpathlist[:split1_idx]+fpathlist[split2_idx:]\n",
    "    val_set = fpathlist[split1_idx:split2_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_fpathseqlist(fpathlist, seq_len, ds_rate=1, n_samples=None):\n",
    "    \"\"\"\n",
    "    Returns a list of list that can be passed to ImageSequenceDataset\n",
    "    # n_samples: int\n",
    "    # between 1 and len(fpathlist)\n",
    "    # If None, it's set to len(fpathlist)/seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_len = seq_len*ds_rate\n",
    "    if n_samples is None:\n",
    "        n_samples = int(len(fpathlist)/seq_len)\n",
    "        sample_stride = sample_len\n",
    "    else:\n",
    "        assert type(n_samples)==int\n",
    "        assert len(fpathlist)>n_samples\n",
    "        sample_stride = int(len(fpathlist)/n_samples)\n",
    "        # for adult group, sample_stride ~=10. i.e. each frame contributes to more than 1 sample sequence, \n",
    "        # but doesn't appear in the same index of the sequence.\n",
    "\n",
    "    fpathseqlist = [fpathlist[i:i+sample_len:ds_rate] \n",
    "                    for i in range(0, n_samples*sample_stride, sample_stride)]\n",
    "    return fpathseqlist\n",
    "\n",
    "def get_fold(gx_fpathlist, fold, max_folds, args):\n",
    "#     fold_size = int(len(gx_fpathlist)/max_folds)\n",
    "    segment_size = int(30*60*30/args.ds_rate)\n",
    "    \n",
    "    fold_segments = []\n",
    "\n",
    "    for i_st in range(0, len(gx_fpathlist), segment_size):\n",
    "        if (i_st // segment_size) % max_folds == fold:\n",
    "            fold_segments.append(gx_fpathlist[i_st:i_st + segment_size])\n",
    "            \n",
    "    fold_segments = list(itertools.chain.from_iterable(fold_segments))\n",
    "    return fold_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0b54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "\n",
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "    transform_list = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "#     Alternative: \n",
    "#     transform_list = [tr.RandomResizedCrop(crop_size, scale=crop_scale),\n",
    "#                     tr.ToTensor(),\n",
    "#                     tr.Normalize(mean,std)]# crop_scale=(0.3, 1.0),\n",
    "    return tr.Compose(transform_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c2d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageSequenceDataset(Dataset):\n",
    "import PIL\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform, shuffle=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "        fp = self.image_paths[idx][0]\n",
    "        images = self.transform(PIL.Image.open(fp))\n",
    "#         images = self.transform(torchvision.io.read_image(fp))\n",
    "#         images = torch.cat([\n",
    "#             self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\n",
    "#                      for fp in self.image_paths[idx]]) #with tochvision transform\n",
    "        \n",
    "#         if self.shuffle:\n",
    "#             size = images.size(0)\n",
    "#             perm = torch.randperm(size)\n",
    "#             images = images[perm]\n",
    "            \n",
    "        return images\n",
    "\n",
    "class ImageSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform, shuffle=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "#         fp = self.image_paths[idx][0]\n",
    "#         images = self.transform(PIL.Image.open(fp))\n",
    "#         images = self.transform(torchvision.io.read_image(fp))\n",
    "        images = torch.cat([\n",
    "            self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\n",
    "                     for fp in self.image_paths[idx]]) #with tochvision transform\n",
    "\n",
    "        if self.shuffle:\n",
    "            size = images.size(0)\n",
    "            perm = torch.randperm(size)\n",
    "            images = images[perm]\n",
    "            \n",
    "        return images\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39803b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from homeview import (_get_transform, get_fpathlist,\n",
    "    get_fold, get_train_val_split, get_fpathseqlist, \n",
    "    get_group,                  \n",
    "    ImageSequenceDataset, ImageDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe4de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(subj_dirs, image_size, args):\n",
    "    seq_len = args.num_frames #kwargs['seq_len']\n",
    "#     n_groupframes=kwargs['n_groupframes']#1450000\n",
    "    ds_rate = args.ds_rate #kwargs['ds_rate']\n",
    "    jpg_root = args.jpg_root #kwargs['jpg_root']\n",
    "#     image_size = kwargs['image_size']\n",
    "    fold = args.fold #kwargs['fold']\n",
    "    condition = args.condition #kwargs['condition']\n",
    "    n_trainsamples = args.n_trainsamples\n",
    "    \n",
    "    crop_size = args.crop_size\n",
    "    crop_scale = args.crop_scale\n",
    "    \n",
    "    transform = _get_transform(image_size)\n",
    "#     make_transforms(\n",
    "#         crop_size=crop_size,\n",
    "#         crop_scale=crop_scale)\n",
    "    #_get_transform(image_size)\n",
    "    gx_fpathlist = []\n",
    "    for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "        gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)\n",
    "    \n",
    "    # added on May15\n",
    "    max_folds = 3\n",
    "    gx_fpathlist = get_fold(gx_fpathlist, fold, max_folds, args)\n",
    "    print('Num. frames in the fold:',len(gx_fpathlist))\n",
    "\n",
    "    #     if len(gx_fpathlist)>=n_groupframes:\n",
    "#         gx_fpathlist = gx_fpathlist[:n_groupframes]\n",
    "#         # 1450000/16 = 90625 => n_trainsamples=81560, n_valsamples= 9060\n",
    "#         # 1274 iterations of train. 141 iterations of test. \n",
    "#         n_trainsamples = None\n",
    "#         n_valsamples = None\n",
    "#     else:\n",
    "    \n",
    "    # Train-val split\n",
    "    gx_train_fp, gx_val_fp = get_train_val_split(gx_fpathlist, val_ratio=0.1)\n",
    "\n",
    "    if condition=='longshuffle':\n",
    "        random.shuffle(gx_train_fp)\n",
    "    \n",
    "    n_trainsamples = n_trainsamples #int(0.9*n_groupframes/seq_len) #81k\n",
    "    \n",
    "    n_maxvalsamples = int(len(gx_val_fp)/seq_len)\n",
    "    n_valsamples = min(n_maxvalsamples, 10000)  #means don't do bootstraping for val. Use whatever number 0.1*len(gx_fpathlist) gives.\n",
    "    \n",
    "    gx_train_fpathseqlist = get_fpathseqlist(gx_train_fp, seq_len, ds_rate=1, n_samples=n_trainsamples)\n",
    "    gx_val_fpathseqlist = get_fpathseqlist(gx_val_fp, seq_len, ds_rate=1, n_samples=n_valsamples)\n",
    "    \n",
    "    \n",
    "    if condition=='shuffle':\n",
    "        train_dataset = ImageSequenceDataset(gx_train_fpathseqlist, transform=transform, shuffle=True)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "    \n",
    "    elif condition=='static':\n",
    "        train_dataset = StillVideoDataset(gx_train_fpathseqlist, transform=transform)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "    elif condition=='image':\n",
    "        train_dataset = ImageDataset(gx_train_fpathseqlist, transform=transform)\n",
    "        val_dataset = ImageDataset(gx_val_fpathseqlist, transform=transform)\n",
    "#         StillVideoDataset(gx_val_fpathseqlist, transform=transform)\n",
    "\n",
    "    else:\n",
    "        train_dataset = ImageSequenceDataset(gx_train_fpathseqlist, transform=transform, shuffle=False)\n",
    "        val_dataset = ImageSequenceDataset(gx_val_fpathseqlist, transform=transform, shuffle=False)\n",
    "        \n",
    "    return {'train':train_dataset,\n",
    "           'val': val_dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d4e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b86ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2107a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size=224\n",
    "crop_scale=(0.3, 1.0)\n",
    "use_gaussian_blur=False\n",
    "use_horizontal_flip=False\n",
    "use_color_distortion=False\n",
    "color_jitter=0\n",
    "patch_size=16\n",
    "pred_mask_scale=(0.15, 0.2)\n",
    "enc_mask_scale=(0.85,1)\n",
    "aspect_ratio=(0.75,1.5)\n",
    "num_enc_masks=1\n",
    "num_pred_masks=4\n",
    "allow_overlap=False\n",
    "min_keep=10\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "jpg_root='/N/project/baby_vision_curriculum/homeview_subset_30fps/'\n",
    "data_seed=401\n",
    "\n",
    "saveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/jul17/'\n",
    "tbsaveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/benchmarks/toybox/jul17/'\n",
    "ucsaveroot='/N/project/baby_vision_curriculum/trained_models/predictive/v0/benchmarks/ucf101/jul17/'\n",
    "\n",
    "n_epoch=1\n",
    "curr='yo'\n",
    "condition='default'#'image'\n",
    "other_id=curr+'_'+condition\n",
    "monitor='grad'\n",
    "ds_rate=30 #1\n",
    "batch_size=8\n",
    "n_trainsamples=12000#162000\n",
    "max_epoch_iters=2000\n",
    "tbbatch_size=64\n",
    "ucbatch_size=64\n",
    "mask_sampler='tube'\n",
    "tubelet_size=1\n",
    "num_frames=4\n",
    "image_size=224\n",
    "\n",
    "train_group='g2'#'g0'\n",
    "fold=data_seed % 3\n",
    "# mask_ratio=0.9\n",
    "#adamw: lr=1.5e-4, wd=0.05, momentum=0.9 (doesn't get used)\n",
    "#adam: lr=0.001. wd=1e-4, momentum=0.9 (doesn't get used)\n",
    "#sgd: 0.1, 0, 0.9\n",
    "optim='sgd'\n",
    "lr=0.1\n",
    "wd=0\n",
    "architecture='base'#'small'#\n",
    "momentum=0.9\n",
    "\n",
    "world_size= 1\n",
    "\n",
    "savedir=saveroot+'s1/' #\"${saveroot}s1/\"\n",
    "# Initialization\n",
    "init_checkpoint_path='na'\n",
    "\n",
    "# other_seed=data_seed#$data_seed\n",
    "script='pretrain_vjepa_v0.py'\n",
    "\n",
    "args = Args(jpg_root=jpg_root,\n",
    "            train_group=train_group,\n",
    "            savedir=savedir,\n",
    "            architecture=architecture,\n",
    "            init_checkpoint_path=init_checkpoint_path,\n",
    "            seed=data_seed,\n",
    "            other_id=other_id,\n",
    "            batch_size=batch_size,\n",
    "           num_frames=num_frames,\n",
    "            tubelet_size=tubelet_size,\n",
    "           ds_rate=ds_rate,\n",
    "           fold=fold,\n",
    "           condition=condition,\n",
    "           n_trainsamples=n_trainsamples,\n",
    "           crop_size=crop_size,\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "           crop_scale=crop_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f055f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['043MP', '044ET', '046TE', '047MS', '048KG', '049JC', '050AB', '050AK', '051DW']\n"
     ]
    }
   ],
   "source": [
    "g0='008MS+009SS+010BF+011EA+012TT+013LS+014SN+015JM+016TF+017EW'\n",
    "g1='026AR+027SS+028CK+028MR+029TT+030FD+031HW+032SR+033SE+034JC'\n",
    "g2='043MP+044ET+046TE+047MS+048KG+049JC+050AB+050AK+051DW'\n",
    "g3='BR+CW+EA+ED+JB+KI+LS+SB+TR'\n",
    "# Total number of frames in each age group: g0=1.68m, g1=1.77m, g2=1.45m\n",
    "\n",
    "g0 = g0.split('+')\n",
    "g1 = g1.split('+')\n",
    "g2 = g2.split('+')\n",
    "g3 = g3.split('+')\n",
    "\n",
    "gRand=[]\n",
    "for gx in [g0,g1,g2,g3]:\n",
    "    gRand.extend(random.sample(gx, 3))\n",
    "random.shuffle(gRand)\n",
    "\n",
    "group_dict = {\"g0\":g0, \"g1\":g1, \"g2\":g2, \"g3\":g3, 'gr':gRand}\n",
    "group = group_dict.get(args.train_group)\n",
    "print(group)                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbb8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:33<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. frames in the fold: 46811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_size=224\n",
    "datasets = make_dataset(group, image_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7caedf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from multiprocessing import Value\n",
    "# from logging import getLogger\n",
    "\n",
    "# _GLOBAL_SEED = 0\n",
    "# logger = getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22fbd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask import MaskCollator as MBMaskCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe1edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_collator = MBMaskCollator(\n",
    "    input_size=crop_size,\n",
    "    patch_size=patch_size,\n",
    "    pred_mask_scale=pred_mask_scale,\n",
    "    enc_mask_scale=enc_mask_scale,\n",
    "    aspect_ratio=aspect_ratio,\n",
    "    nenc=num_enc_masks,\n",
    "    npred=num_pred_masks,\n",
    "    allow_overlap=allow_overlap,\n",
    "    min_keep=min_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17e05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory = False\n",
    "# num_workers=\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "        datasets[x], batch_size=batch_size, pin_memory=pin_memory, \n",
    "        shuffle=False, collate_fn=mask_collator,\n",
    "        drop_last=True)\n",
    "                        for x in ['train', 'val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3314b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ceb954a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/750 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "for inputs in tqdm(dataloaders[phase]):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ce8ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "udata: torch.Size([6, 3, 224, 224])\n",
      "encoder context: torch.Size([16, 48])\n",
      "prediction target: torch.Size([16, 36])\n",
      "prediction target: torch.Size([16, 36])\n",
      "prediction target: torch.Size([16, 36])\n",
      "prediction target: torch.Size([16, 36])\n"
     ]
    }
   ],
   "source": [
    "for item in inputs[0]:\n",
    "    print('udata:',item.shape)\n",
    "for item in inputs[1]:\n",
    "    print('encoder context:',item.shape)\n",
    "for item in inputs[2]:\n",
    "    print('prediction target:',item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac8a0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vision_transformer as vit\n",
    "from tensors import trunc_normal_\n",
    "\n",
    " #@@@\n",
    "def init_model(\n",
    "    device,\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    tubelet_size=1,\n",
    "    num_frames=1,\n",
    "    model_name='vit_base',\n",
    "    pred_depth=6,\n",
    "    pred_emb_dim=384\n",
    "):\n",
    "#     encoder = vit_base(\n",
    "    encoder = vit.__dict__[model_name](\n",
    "        img_size=[image_size],\n",
    "        patch_size=patch_size,\n",
    "        num_frames=num_frames,\n",
    "        tubelet_size=tubelet_size) #@@@\n",
    "#     predictor = vit_predictor(\n",
    "    predictor = vit.__dict__['vit_predictor'](\n",
    "        sequence_shape=encoder.sequence_shape,\n",
    "        embed_dim=encoder.embed_dim,\n",
    "        predictor_embed_dim=pred_emb_dim,\n",
    "        depth=pred_depth,\n",
    "        num_heads=encoder.num_heads)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    for m in encoder.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    for m in predictor.modules():\n",
    "        init_weights(m)\n",
    "\n",
    "    encoder.to(device)\n",
    "    predictor.to(device)\n",
    "    logger.info(encoder)\n",
    "    return encoder, predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e64ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import init_opt #init_model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "226b92e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(3, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate=none)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device= 'cuda:0'#'cpu'\n",
    "model_name='vit_'+args.architecture #'vit_base'#'vit_small'\n",
    "pred_depth=6\n",
    "pred_emb_dim=384\n",
    "# tubelet_size=2#2#1 #@@@\n",
    "# num_frames=6#6#1 #@@@\n",
    "\n",
    "encoder, predictor = init_model(\n",
    "    device=device,\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_emb_dim=pred_emb_dim,\n",
    "    model_name=model_name,\n",
    "    tubelet_size=tubelet_size,\n",
    "    num_frames=num_frames)\n",
    "target_encoder = deepcopy(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90274a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the masks are defined for certain spatial\n",
    "# patches. Their index is based on single frame.\n",
    "# in order to apply them to spatiotemporal patches, we need to convert \n",
    "# their indices. In particular, for target indices where we want to\n",
    "# choose from future frames, we might want to add a random multiple\n",
    "# of num_patches_per_frame to all masks. the random integer is in [0, num_frames-1]\n",
    "\n",
    "def update_masks(masks, args):\n",
    "    T = args.num_frames//args.tubelet_size\n",
    "    num_patches_per_frame = (args.image_size//args.patch_size)**2\n",
    "    \n",
    "    for i_mask, m in enumerate(masks):\n",
    "        frame_index = np.random.randint(0,T)\n",
    "        m+=frame_index*num_patches_per_frame\n",
    "        masks[i_mask] = m\n",
    "    return masks\n",
    "\n",
    "\n",
    "def apply_masks(x, masks):\n",
    "    \"\"\"\n",
    "    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n",
    "    :param masks: list of tensors containing indices of patches in [N] to keep\n",
    "    \"\"\"\n",
    "    all_x = []\n",
    "    for m in masks:\n",
    "        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n",
    "        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n",
    "    return torch.cat(all_x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7d54c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Using SGD\n"
     ]
    }
   ],
   "source": [
    "wd=0\n",
    "final_wd=0\n",
    "start_lr=0.1\n",
    "ref_lr=0.1\n",
    "final_lr=0.1\n",
    "ipe=2000 #iterations_per_epoch\n",
    "warmup=0\n",
    "num_epochs=1\n",
    "ipe_scale=1.0\n",
    "use_bfloat16=True\n",
    "\n",
    "optimizer, scaler, scheduler, wd_scheduler = init_opt(\n",
    "    encoder=encoder,\n",
    "    predictor=predictor,\n",
    "    wd=wd,\n",
    "    final_wd=final_wd,\n",
    "    start_lr=start_lr,\n",
    "    ref_lr=lr,\n",
    "    final_lr=final_lr,\n",
    "    iterations_per_epoch=ipe,\n",
    "    warmup=warmup,\n",
    "    num_epochs=num_epochs,\n",
    "    ipe_scale=ipe_scale,\n",
    "    use_bfloat16=use_bfloat16)\n",
    "# encoder = DistributedDataParallel(encoder, static_graph=True)\n",
    "# predictor = DistributedDataParallel(predictor, static_graph=True)\n",
    "# target_encoder = DistributedDataParallel(target_encoder)\n",
    "for p in target_encoder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d76daa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- momentum schedule\n",
    "ema = (0.996, 1.0)\n",
    "momentum_scheduler = (ema[0] + i*(ema[1]-ema[0])/(ipe*num_epochs*ipe_scale)\n",
    "                      for i in range(int(ipe*num_epochs*ipe_scale)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86b7b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def save_checkpoint(epoch):\n",
    "#     save_dict = {\n",
    "#         'encoder': encoder.state_dict(),\n",
    "#         'predictor': predictor.state_dict(),\n",
    "#         'target_encoder': target_encoder.state_dict(),\n",
    "#         'opt': None,\n",
    "#         'scaler': None if scaler is None else scaler.state_dict(),\n",
    "#         'epoch': epoch,\n",
    "#         'loss': loss_meter.avg,\n",
    "#         'batch_size': batch_size,\n",
    "#         'world_size': world_size,\n",
    "#         'lr': lr\n",
    "#     }\n",
    "#     if rank == 0:\n",
    "#         torch.save(save_dict, latest_path)\n",
    "#         if (epoch + 1) % checkpoint_freq == 0:\n",
    "#             torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=0\n",
    "r_file=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d57d20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loggingtools import (\n",
    "    CSVLogger,\n",
    "    grad_logger,\n",
    "    AverageMeter,\n",
    "    gpu_timer)\n",
    "\n",
    "import yaml\n",
    "\n",
    "# -- LOGGING\n",
    "folder = '/N/project/baby_vision_curriculum/trained_models/predictive/v0/jul24/'\n",
    "#args['logging']['folder']\n",
    "tag = 'jepa'\n",
    "#args['logging']['write_tag']\n",
    "\n",
    "dump = os.path.join(folder, 'params-ijepa.yaml')\n",
    "with open(dump, 'w') as f:\n",
    "    yaml.dump(args, f)\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "# try:\n",
    "#     mp.set_start_method('spawn')\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# -- init torch distributed backend\n",
    "# world_size, rank = init_distributed()\n",
    "# logger.info(f'Initialized (rank/world-size) {rank}/{world_size}')\n",
    "# if rank > 0:\n",
    "#     logger.setLevel(logging.ERROR)\n",
    "\n",
    "# -- log/checkpointing paths\n",
    "log_file = os.path.join(folder, f'{tag}_r{rank}.csv')\n",
    "save_path = os.path.join(folder, f'{tag}' + '-ep{epoch}.pth.tar')\n",
    "latest_path = os.path.join(folder, f'{tag}-latest.pth.tar') #save checkpoint path\n",
    "load_path = None\n",
    "load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
    "\n",
    "# -- make csv_logger\n",
    "csv_logger = CSVLogger(log_file,\n",
    "                       ('%d', 'epoch'),\n",
    "                       ('%d', 'itr'),\n",
    "                       ('%.5f', 'loss'),\n",
    "                       ('%.5f', 'mask-A'),\n",
    "                       ('%.5f', 'mask-B'),\n",
    "                       ('%d', 'time (ms)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c78cad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/N/project/baby_vision_curriculum/trained_models/predictive/v0/jul24/jepa-latest.pth.tar\n"
     ]
    }
   ],
   "source": [
    "print(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b06ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tensors import apply_masks, repeat_interleave_batch\n",
    "from distributed import AllReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6d648a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    save_dict = {\n",
    "        'encoder': encoder.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'target_encoder': target_encoder.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss_meter.avg,\n",
    "        'batch_size': batch_size,\n",
    "        'world_size': world_size,\n",
    "        'lr': lr\n",
    "    }\n",
    "    if rank == 0:\n",
    "        torch.save(save_dict, latest_path)\n",
    "#         if (epoch + 1) % checkpoint_freq == 0:\n",
    "#             torch.save(save_dict, save_path.format(epoch=f'{epoch + 1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a4cb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:[1,     0] loss: 0.169 masks: 73.0 30.0 [mem: 2.42e+03] (248.4 ms)\n",
      "INFO:root:[1,     0] grad_stats: [2.85e-03 1.26e-03] (1.26e-03, 3.01e-03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                        | 10/1500 [00:04<10:08,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:[1,    10] loss: 0.186 masks: 62.0 33.7 [mem: 2.45e+03] (242.7 ms)\n",
      "INFO:root:[1,    10] grad_stats: [1.35e-03 8.26e-04] (8.16e-04, 2.22e-03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                        | 12/1500 [00:05<10:20,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:avg. loss 0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "start_epoch = 0\n",
    "\n",
    "# --\n",
    "log_timings = True\n",
    "log_freq = 10\n",
    "checkpoint_freq = 50\n",
    "# --\n",
    "\n",
    "load_model = False #True #False\n",
    "# -- load training checkpoint\n",
    "if load_model:\n",
    "    encoder, predictor, target_encoder, optimizer, scaler, start_epoch = load_checkpoint(\n",
    "        r_path=load_path,\n",
    "        encoder=encoder,\n",
    "        predictor=predictor,\n",
    "        target_encoder=target_encoder,\n",
    "        opt=optimizer,\n",
    "        scaler=scaler)\n",
    "    for _ in range(start_epoch*ipe):\n",
    "        mask_collator.step()\n",
    "#     start_epoch+=1\n",
    "#         scheduler.step()\n",
    "#         wd_scheduler.step()\n",
    "#         next(momentum_scheduler)\n",
    "        \n",
    "\n",
    "            \n",
    "# -- TRAINING LOOP\n",
    "for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "    logger.info('Epoch %d' % (epoch + 1))\n",
    "\n",
    "    # -- update distributed-data-loader epoch\n",
    "#     unsupervised_sampler.set_epoch(epoch)\n",
    "\n",
    "    loss_meter = AverageMeter()\n",
    "    maskA_meter = AverageMeter()\n",
    "    maskB_meter = AverageMeter()\n",
    "    time_meter = AverageMeter()\n",
    "\n",
    "    for itr, (udata, masks_enc, masks_pred) in enumerate(\n",
    "        tqdm(dataloaders[phase])):\n",
    "        \n",
    "        i_break = 11\n",
    "        if itr>i_break:\n",
    "            break #@@@\n",
    "        masks_enc = update_masks(masks_enc, args)\n",
    "        masks_pred = update_masks(masks_pred, args)\n",
    "        \n",
    "        def load_imgs():\n",
    "            # -- unsupervised imgs\n",
    "            imgs = udata.to(device, non_blocking=True)\n",
    "            masks_1 = [u.to(device, non_blocking=True) for u in masks_enc]\n",
    "            masks_2 = [u.to(device, non_blocking=True) for u in masks_pred]\n",
    "            return (imgs, masks_1, masks_2)\n",
    "        imgs, masks_enc, masks_pred = load_imgs()\n",
    "        maskA_meter.update(len(masks_enc[0][0]))\n",
    "        maskB_meter.update(len(masks_pred[0][0]))\n",
    "\n",
    "        def train_step():\n",
    "#             _new_lr = scheduler.step()\n",
    "#             _new_wd = wd_scheduler.step()\n",
    "            # --\n",
    "\n",
    "            def forward_target():\n",
    "                with torch.no_grad():\n",
    "                    h = target_encoder(imgs)\n",
    "                    h = F.layer_norm(h, (h.size(-1),))  # normalize over feature-dim\n",
    "                    B = len(h)\n",
    "                    # -- create targets (masked regions of h)\n",
    "                    h = apply_masks(h, masks_pred)\n",
    "#                     print('h shape:', h.shape)\n",
    "                    h = repeat_interleave_batch(h, B, repeat=len(masks_enc))\n",
    "                    return h\n",
    "\n",
    "            def forward_context():\n",
    "                z = encoder(imgs, masks_enc)\n",
    "                z = predictor(z, masks_enc, masks_pred)\n",
    "                return z\n",
    "\n",
    "            def loss_fn(z, h):\n",
    "                loss = F.smooth_l1_loss(z, h)\n",
    "                loss = AllReduce.apply(loss)\n",
    "                return loss\n",
    "\n",
    "            # Step 1. Forward\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16, \n",
    "                                         enabled=use_bfloat16):\n",
    "                h = forward_target()\n",
    "                z = forward_context()\n",
    "                loss = loss_fn(z, h)\n",
    "\n",
    "            #  Step 2. Backward & step\n",
    "            if phase == 'val':\n",
    "                return float(loss),0\n",
    "            \n",
    "            if use_bfloat16:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            grad_stats = grad_logger(encoder.named_parameters())\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3. momentum update of target encoder\n",
    "            with torch.no_grad():\n",
    "                m = next(momentum_scheduler)\n",
    "                for param_q, param_k in zip(encoder.parameters(), target_encoder.parameters()):\n",
    "                    param_k.data.mul_(m).add_((1.-m) * param_q.detach().data)\n",
    "            return (float(loss), grad_stats)\n",
    "            \n",
    "        if phase=='val':\n",
    "            (loss, _), etime = gpu_timer(train_step)\n",
    "            logger.info('val loss: %.3f etime %.1f ms', loss, etime)\n",
    "            continue\n",
    "        (loss, grad_stats), etime = gpu_timer(train_step)\n",
    "        loss_meter.update(loss)\n",
    "        time_meter.update(etime)\n",
    "\n",
    "        # -- Logging\n",
    "        def log_stats():\n",
    "            csv_logger.log(epoch + 1, itr, loss, maskA_meter.val, maskB_meter.val, etime)\n",
    "            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
    "                logger.info('[%d, %5d] loss: %.3f '\n",
    "                            'masks: %.1f %.1f '\n",
    "                            '[mem: %.2e] '\n",
    "                            '(%.1f ms)'\n",
    "                            % (epoch + 1, itr,\n",
    "                               loss_meter.avg,\n",
    "                               maskA_meter.avg,\n",
    "                               maskB_meter.avg,\n",
    "                               torch.cuda.max_memory_allocated() / 1024.**2,\n",
    "                               time_meter.avg))\n",
    "\n",
    "                if grad_stats is not None:\n",
    "                    logger.info('[%d, %5d] grad_stats: [%.2e %.2e] (%.2e, %.2e)'\n",
    "                                % (epoch + 1, itr,\n",
    "                                   grad_stats.first_layer,\n",
    "                                   grad_stats.last_layer,\n",
    "                                   grad_stats.min,\n",
    "                                   grad_stats.max))\n",
    "\n",
    "        log_stats()\n",
    "\n",
    "        assert not np.isnan(loss), 'loss is nan'\n",
    "\n",
    "    # -- Save Checkpoint after every epoch\n",
    "    logger.info('avg. loss %.3f' % loss_meter.avg)\n",
    "    save_checkpoint(epoch+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
