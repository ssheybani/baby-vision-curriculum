{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1683544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# added in v1:\n",
    "# New arguments:\n",
    "# jpg_root\n",
    "# architecture choice \n",
    "# lr, batch_size\n",
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)\n",
    "\n",
    "# SCRIPT_DIR = os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe()))) #os.getcwd() #\n",
    "# print('cwd: ',SCRIPT_DIR)\n",
    "#os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe())))\n",
    "# util_path = os.path.normpath(os.path.join(SCRIPT_DIR, '..', 'util'))\n",
    "# sys.path.insert(0, util_path)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '38' #@@@@ to help with the num_workers issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  #10\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "import os\n",
    "# import random\n",
    "# import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from ddputils import is_main_process, save_on_master, setup_for_distributed\n",
    "import transformers\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist\n",
    "\n",
    "\n",
    "class ImageSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform):\n",
    "        # transform is a Hugging Face image processor transform. check the usage in __getitem\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "        images = torch.cat([\n",
    "            self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\n",
    "                     for fp in self.image_paths[idx]]) #with tochvision transform\n",
    "#         images = [Image.open(fp) for fp in self.image_paths[idx]]\n",
    "#         images = self.transform(images, return_tensors=\"pt\").pixel_values[0] #with VideoMAEImageProcessor\n",
    "        return images\n",
    "    \n",
    "def get_train_val_split(fpathlist, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the list of filepaths into a train list and test list\n",
    "    \"\"\"\n",
    "    n_fr = len(fpathlist)\n",
    "    val_size = int(n_fr*val_ratio)\n",
    "    \n",
    "    split1_idx = int((n_fr-val_size)/2)\n",
    "    split2_idx = int((n_fr+val_size)/2)\n",
    "    train_set =fpathlist[:split1_idx]+fpathlist[split2_idx:]\n",
    "    val_set = fpathlist[split1_idx:split2_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_fpathseqlist(fpathlist, seq_len, ds_rate=1, n_samples=None):\n",
    "    \"\"\"\n",
    "    Returns a list of list that can be passed to ImageSequenceDataset\n",
    "    # n_samples: int\n",
    "    # between 1 and len(fpathlist)\n",
    "    # If None, it's set to len(fpathlist)/seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_len = seq_len*ds_rate\n",
    "    if n_samples is None:\n",
    "        n_samples = int(len(fpathlist)/seq_len)\n",
    "        sample_stride = sample_len\n",
    "    else:\n",
    "        assert type(n_samples)==int\n",
    "        sample_stride = int(len(fpathlist)/n_samples)\n",
    "\n",
    "    fpathseqlist = [fpathlist[i:i+sample_len:ds_rate] \n",
    "                    for i in range(0, n_samples*sample_stride, sample_stride)]\n",
    "    return fpathseqlist\n",
    "\n",
    "\n",
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "    \n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)\n",
    "\n",
    "# def _get_transform(image_size):\n",
    "# # image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "#     image_processor = transformers.VideoMAEImageProcessor(\n",
    "#         size={\"shortest_edge\":image_size},\n",
    "#         do_center_crop=True, crop_size={\"height\":image_size, \"width\": image_size}\n",
    "#     )\n",
    "#     return image_processor\n",
    "\n",
    "\n",
    "\n",
    "def make_dataset(subj_dirs, **kwargs):\n",
    "    seq_len = kwargs['seq_len']\n",
    "    n_groupframes=kwargs['n_groupframes']#1450000\n",
    "    ds_rate = kwargs['ds_rate']\n",
    "    jpg_root = kwargs['jpg_root']\n",
    "    image_size = kwargs['image_size']\n",
    "    transform = _get_transform(image_size)\n",
    "    gx_fpathlist = []\n",
    "    for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "        gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)\n",
    "    gx_fpathlist = gx_fpathlist[:n_groupframes]\n",
    "\n",
    "    # Train-val split\n",
    "    gx_train_fp, gx_val_fp = get_train_val_split(gx_fpathlist, val_ratio=0.1)\n",
    "\n",
    "\n",
    "    gx_train_fpathseqlist = get_fpathseqlist(gx_train_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    gx_val_fpathseqlist = get_fpathseqlist(gx_val_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    \n",
    "    return {'train':ImageSequenceDataset(gx_train_fpathseqlist, transform=transform),\n",
    "           'val': ImageSequenceDataset(gx_val_fpathseqlist, transform=transform)}\n",
    "\n",
    "def get_config(image_size, args):\n",
    "    arch_kw = args.architecture\n",
    "    \n",
    "    if len(arch_kw)==0: #default\n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n",
    "                                             intermediate_size=3072, initializer_range=0.02,\n",
    "                                             use_mean_pooling=True, decoder_num_attention_heads=6,\n",
    "                                             decoder_hidden_size=384, decoder_num_hidden_layers=4, \n",
    "                                             decoder_intermediate_size=1536, norm_pix_loss=True)\n",
    "    elif arch_kw=='small1':\n",
    "        hidden_size = 384\n",
    "        intermediate_size = 4*384\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 12\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size)\n",
    "        \n",
    "    elif arch_kw=='small2':\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 4*768\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size)\n",
    "    \n",
    "    return config\n",
    "    \n",
    "    \n",
    "def get_model(image_size, args):\n",
    "    config = get_config(image_size, args)\n",
    "    model = transformers.VideoMAEForPreTraining(config)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8896c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.VideoMAEConfig(image_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a28b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodel = transformers.VideoMAEForPreTraining(config)#get_model(image_size, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc271bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 16,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": true\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmodel.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462e5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=1\n",
    "xmodel = xmodel.to(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b2f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8066bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(rank)    \n\u001b[0;32m----> 3\u001b[0m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36msetup\u001b[0;34m(rank, world_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12355\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:595\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    592\u001b[0m rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[1;32m    593\u001b[0m     init_method, rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    594\u001b[0m )\n\u001b[0;32m--> 595\u001b[0m store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/distributed/rendezvous.py:257\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m master_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_get_env_or_raise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 257\u001b[0m store \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c10d_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m (store, rank, world_size)\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/distributed/rendezvous.py:188\u001b[0m, in \u001b[0;36m_create_c10d_store\u001b[0;34m(hostname, port, rank, world_size, timeout)\u001b[0m\n\u001b[1;32m    187\u001b[0m start_daemon \u001b[38;5;241m=\u001b[39m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTCPStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhostname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_daemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_tenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Interrupted system call",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "world_size=4\n",
    "torch.cuda.set_device(rank)    \n",
    "setup(rank, world_size) # setup the process groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cfc3a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(\"model device\", xmodel.device)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDDP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mfind_unused_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:595\u001b[0m, in \u001b[0;36mDistributedDataParallel.__init__\u001b[0;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view, static_graph)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device \u001b[38;5;241m=\u001b[39m _get_device_index(output_device, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m process_group\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:429\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n",
    "    # print(\"model device\", xmodel.device)\n",
    "xmodel = DDP(xmodel, device_ids=[rank], output_device=rank, \n",
    "               find_unused_parameters=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf63189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
