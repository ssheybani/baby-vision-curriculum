{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b73301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages'\n",
    "# Himanshu: '/N/slate/hhansar/hgenv/lib/python3.10/site-packages'\n",
    "sys.path.insert(0,env_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688632bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6934cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f908dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87b0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeMaskingGenerator:\n",
    "    def __init__(self, input_size, mask_ratio):\n",
    "        self.frames, self.height, self.width = input_size\n",
    "        self.num_patches_per_frame =  self.height * self.width\n",
    "        self.total_patches = self.frames * self.num_patches_per_frame \n",
    "        self.num_masks_per_frame = int(mask_ratio * self.num_patches_per_frame)\n",
    "        self.total_masks = self.frames * self.num_masks_per_frame\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = \"Maks: total patches {}, mask patches {}\".format(\n",
    "            self.total_patches, self.total_masks\n",
    "        )\n",
    "        return repr_str\n",
    "\n",
    "    def __call__(self):\n",
    "        mask_per_frame = np.hstack([\n",
    "            np.zeros(self.num_patches_per_frame - self.num_masks_per_frame),\n",
    "            np.ones(self.num_masks_per_frame),\n",
    "        ])\n",
    "        np.random.shuffle(mask_per_frame)\n",
    "        mask = np.tile(mask_per_frame, (self.frames,1)).flatten()\n",
    "        return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = int(16/2), int(224/16), int(224/16)\n",
    "mask_ratio = 0.9\n",
    "Mask_gen = TubeMaskingGenerator(input_size, mask_ratio)\n",
    "xtt = Mask_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e886a48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1568,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12652b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist\n",
    "\n",
    "\n",
    "class ImageSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform):\n",
    "        # transform is a Hugging Face image processor transform. check the usage in __getitem\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "#         images = [Image.open(fp) for fp in self.image_paths[idx]]\n",
    "        images = torch.cat([\n",
    "            self.transform(torchvision.io.read_image(fp)).unsqueeze(0)\n",
    "                     for fp in self.image_paths[idx]])\n",
    "#         images = self.transform(images, return_tensors=\"pt\").pixel_values[0]\n",
    "        return images\n",
    "    \n",
    "def get_train_val_split(fpathlist, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the list of filepaths into a train list and test list\n",
    "    \"\"\"\n",
    "    n_fr = len(fpathlist)\n",
    "    val_size = int(n_fr*val_ratio)\n",
    "    \n",
    "    split1_idx = int((n_fr-val_size)/2)\n",
    "    split2_idx = int((n_fr+val_size)/2)\n",
    "    train_set =fpathlist[:split1_idx]+fpathlist[split2_idx:]\n",
    "    val_set = fpathlist[split1_idx:split2_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_fpathseqlist(fpathlist, seq_len, ds_rate=1, n_samples=None):\n",
    "    \"\"\"\n",
    "    Returns a list of list that can be passed to ImageSequenceDataset\n",
    "    # n_samples: int\n",
    "    # between 1 and len(fpathlist)\n",
    "    # If None, it's set to len(fpathlist)/seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_len = seq_len*ds_rate\n",
    "    if n_samples is None:\n",
    "        n_samples = int(len(fpathlist)/seq_len)\n",
    "        sample_stride = sample_len\n",
    "    else:\n",
    "        assert type(n_samples)==int\n",
    "        sample_stride = int(len(fpathlist)/n_samples)\n",
    "\n",
    "    fpathseqlist = [fpathlist[i:i+sample_len:ds_rate] \n",
    "                    for i in range(0, n_samples*sample_stride, sample_stride)]\n",
    "    return fpathseqlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7245711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_root='/N/project/infant_image_statistics/preproc_saber/JPG_10fps/'\n",
    "ds_rate = 1\n",
    "\n",
    "n_groupframes = 1450000 # minimum number of frames across age groups\n",
    "\n",
    "g0='008MS+009SS_withrotation+010BF_withrotation+011EA_withrotation+012TT_withrotation+013LS+014SN+015JM+016TF+017EW_withrotation'\n",
    "g1='026AR+027SS+028CK+028MR+029TT+030FD+031HW+032SR+033SE+034JC_withlighting'\n",
    "g2='043MP+044ET+046TE+047MS+048KG+049JC+050AB+050AK_rotation+051DW'\n",
    "# Total number of frames in each age group: g0=1.68m, g1=1.77m, g2=1.45m\n",
    "\n",
    "g0 = g0.split('+')\n",
    "g1 = g1.split('+')\n",
    "g2 = g2.split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1060fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "num_frames = 16\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "image_processor = transformers.VideoMAEImageProcessor(\n",
    "    size={\"shortest_edge\":image_size},\n",
    "    do_center_crop=True, crop_size={\"height\":image_size, \"width\": image_size}\n",
    ")\n",
    "\n",
    "seq_len = num_frames #equivalent to num_frames in VideoMAE()\n",
    "#     ds_rate = 1\n",
    "n_samples = None#10 #50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ecf2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(subj_dirs, **kwargs):\n",
    "    seq_len = kwargs['seq_len']\n",
    "    n_groupframes=kwargs['n_groupframes']#1450000\n",
    "    ds_rate = kwargs['ds_rate']\n",
    "    jpg_root = kwargs['jpg_root']\n",
    "    \n",
    "    gx_fpathlist = []\n",
    "    for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "        gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)\n",
    "    gx_fpathlist = gx_fpathlist[:n_groupframes]\n",
    "\n",
    "    # Train-val split\n",
    "    gx_train_fp, gx_val_fp = get_train_val_split(gx_fpathlist, val_ratio=0.1)\n",
    "\n",
    "\n",
    "    gx_train_fpathseqlist = get_fpathseqlist(gx_train_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    gx_val_fpathseqlist = get_fpathseqlist(gx_val_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    \n",
    "    return {'train':ImageSequenceDataset(gx_train_fpathseqlist, transform=transform),\n",
    "           'val': ImageSequenceDataset(gx_val_fpathseqlist, transform=transform)}\n",
    "#     return gx_val_fpathseqlist #@@@\n",
    "#     return {'train':ImageSequenceDataset(gx_train_fpathseqlist, transform=image_processor),\n",
    "#            'val': ImageSequenceDataset(gx_val_fpathseqlist, transform=image_processor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c103af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xds = make_dataset(g2, seq_len=seq_len, jpg_root=jpg_root, ds_rate=ds_rate, n_groupframes=n_groupframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec0406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xseq= xds[0]\n",
    "# ximages = np.asarray([Image.open(fp) for fp in xseq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ccb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aeb6bb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 480, 640])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ximages = [Image.open(fp)\n",
    "#                      for fp in xseq]\n",
    "\n",
    "# ximages = np.asarray([cv2.imread(fp)\n",
    "#                      for fp in xseq])\n",
    "# ximages = np.asarray([skimage.io.imread(fp)\n",
    "#                      for fp in xseq])\n",
    "ximages = torch.cat([torchvision.io.read_image(fp).unsqueeze(0)\n",
    "                     for fp in xseq])\n",
    "ximages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a01e94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ximagesp = image_processor(ximages, return_tensors=\"pt\").pixel_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6056299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ximagesp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "790f0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef95bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e057091",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ximages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24504faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ximagesp2 = transform(ximages[0,...])\n",
    "ximagesp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "645acb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:14<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets = make_dataset(g2, seq_len=seq_len, jpg_root=jpg_root, ds_rate=ds_rate, n_groupframes=n_groupframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24a72d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt = datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0165aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2#1\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "        datasets[x], batch_size=batch_size)\n",
    "                        for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb954915",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "num_frames=16\n",
    "hidden_size = 768 #384\n",
    "intermediate_size = 3072 #4*384\n",
    "num_atention_heads = 12 #6\n",
    "\n",
    "config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                     num_frames=num_frames, tubelet_size=2, \n",
    "                                     hidden_size=hidden_size, num_hidden_layers=12, num_attention_heads=num_atention_heads,\n",
    "                                     intermediate_size=intermediate_size, initializer_range=0.02,\n",
    "                                     use_mean_pooling=True, decoder_num_attention_heads=6,\n",
    "                                     decoder_hidden_size=384, decoder_num_hidden_layers=4, \n",
    "                                     decoder_intermediate_size=1536, norm_pix_loss=True)\n",
    "# config\n",
    "model = transformers.VideoMAEForPreTraining(config)\n",
    "#model components: base_model, encoder_to_decoder, decoder\n",
    "# model.videomae==model.base_model\n",
    "# model.videomae.embeddings, model.videomae.encoder\n",
    "\n",
    "# embeddings: a Conv3D layer: Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
    "# Splits the image sequence into n tubes \n",
    "# Maps each tube (2x16x16) to a 768D vector using linear projection (one projector for all tubes)\n",
    "# Returns a tensor of shape (789x768) for each image sequence.\n",
    " \n",
    "# encoder: has 12 layers of type VideoMAELayer. Each layer is: [attention, linear, linear(768,3073), gelu, linear(3072,368), layernorm]\n",
    "\n",
    "# encoder_to_decoder: one linear layer: (in_features=768, out_features=384, bias=False)\n",
    "\n",
    "# decoder: has 4 VideoMAELayer layers + a linear projection from 384 to 1536 dimensions\n",
    "\n",
    "\n",
    "# If you use the VideoMAEModel, it only includes the base model (encoder). \n",
    "# You may or may not choose to pass in bool_masked_pos argument.\n",
    "# If you don't, the output is of shape: [1568, 768] (the eoncoding of all tubes?)\n",
    "# If you do, the output is of a different shape: [789, 768] (the predictions for the masked tubes?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee808667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.max = float('-inf')\n",
    "        self.min = float('inf')\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        try:\n",
    "            self.max = max(val, self.max)\n",
    "            self.min = min(val, self.min)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def grad_logger(named_params):\n",
    "    stats = AverageMeter()\n",
    "    stats.enc_first_layer = None\n",
    "    stats.enc_last_layer = None\n",
    "    stats.dec_last_layer = None\n",
    "    \n",
    "    for n, p in named_params:\n",
    "        if (p.grad is not None):\n",
    "            if n=='videomae.embeddings.patch_embeddings.projection.weight':\n",
    "                grad_norm = float(torch.norm(p.grad.data))\n",
    "                stats.update(grad_norm)\n",
    "                stats.enc_first_layer = grad_norm\n",
    "            elif n=='encoder_to_decoder.weight':\n",
    "                grad_norm = float(torch.norm(p.grad.data))\n",
    "                stats.update(grad_norm)\n",
    "                stats.enc_last_layer = grad_norm\n",
    "            elif n=='decoder.head.weight':\n",
    "                grad_norm = float(torch.norm(p.grad.data))\n",
    "                stats.update(grad_norm)\n",
    "                stats.dec_last_layer = grad_norm\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aec34d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "grad_stats = grad_logger(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b7d5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c3e79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                 | 0/40781 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/40781 [00:04<?, ?it/s]\n",
      "  0%|                                                  | 0/4531 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, -1, 768]' is invalid for input of size 1208064",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m bool_masked_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, (batch_size, model_seq_length))\u001b[38;5;241m.\u001b[39mbool()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/transformers/models/videomae/modeling_videomae.py:819\u001b[0m, in \u001b[0;36mVideoMAEForPreTraining.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03mbool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m    Boolean masked positions. Indicates which patches are masked (1) and which aren't (0). Each video in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m>>> loss = outputs.loss\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 819\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideomae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    829\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_to_decoder(\n\u001b[1;32m    830\u001b[0m     sequence_output\n\u001b[1;32m    831\u001b[0m )  \u001b[38;5;66;03m# [batch_size, num_visible_patches, decoder_hidden_size]\u001b[39;00m\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/transformers/models/videomae/modeling_videomae.py:658\u001b[0m, in \u001b[0;36mVideoMAEModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    656\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 658\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_masked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    661\u001b[0m     embedding_output,\n\u001b[1;32m    662\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    667\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/transformers/models/videomae/modeling_videomae.py:148\u001b[0m, in \u001b[0;36mVideoMAEEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[1;32m    146\u001b[0m     batch_size, _, num_channels \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    147\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings[\u001b[38;5;241m~\u001b[39mbool_masked_pos]\n\u001b[0;32m--> 148\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, -1, 768]' is invalid for input of size 1208064"
     ]
    }
   ],
   "source": [
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "model_seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n",
    "    \n",
    "    \n",
    "for phase in ['train', 'val']:\n",
    "#     dataloaders_dict[phase].sampler.set_epoch(i_ep)\n",
    "    if phase == 'train':\n",
    "        model.train()  # Set model to training mode\n",
    "    else:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs in tqdm(dataloaders[phase]):\n",
    "        print(inputs.shape)\n",
    "#         break\n",
    "        optimizer.zero_grad()\n",
    "        bool_masked_pos = torch.randint(0, 2, (batch_size, model_seq_length)).bool()\n",
    "        outputs = model(inputs, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b411e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
