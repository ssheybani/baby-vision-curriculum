{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b73301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "env_root = '/N/slate/sheybani/pythonenvs/hfenv2/lib/python3.10/site-packages'\n",
    "# Himanshu: '/N/slate/hhansar/hgenv/lib/python3.10/site-packages'\n",
    "sys.path.insert(0,env_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688632bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoImageProcessor, VideoMAEForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6934cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12652b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist\n",
    "\n",
    "\n",
    "class ImageSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    To use for video models. \n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, transform):\n",
    "        # transform is a Hugging Face image processor transform. check the usage in __getitem\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the sequence of images\n",
    "        images = [Image.open(fp) for fp in self.image_paths[idx]]\n",
    "        images = self.transform(images, return_tensors=\"pt\").pixel_values[0]\n",
    "        return images\n",
    "    \n",
    "def get_train_val_split(fpathlist, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the list of filepaths into a train list and test list\n",
    "    \"\"\"\n",
    "    n_fr = len(fpathlist)\n",
    "    val_size = int(n_fr*val_ratio)\n",
    "    \n",
    "    split1_idx = int((n_fr-val_size)/2)\n",
    "    split2_idx = int((n_fr+val_size)/2)\n",
    "    train_set =fpathlist[:split1_idx]+fpathlist[split2_idx:]\n",
    "    val_set = fpathlist[split1_idx:split2_idx]\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_fpathseqlist(fpathlist, seq_len, ds_rate=1, n_samples=None):\n",
    "    \"\"\"\n",
    "    Returns a list of list that can be passed to ImageSequenceDataset\n",
    "    # n_samples: int\n",
    "    # between 1 and len(fpathlist)\n",
    "    # If None, it's set to len(fpathlist)/seq_len\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_len = seq_len*ds_rate\n",
    "    if n_samples is None:\n",
    "        n_samples = int(len(fpathlist)/seq_len)\n",
    "        sample_stride = sample_len\n",
    "    else:\n",
    "        assert type(n_samples)==int\n",
    "        sample_stride = int(len(fpathlist)/n_samples)\n",
    "\n",
    "    fpathseqlist = [fpathlist[i:i+sample_len:ds_rate] \n",
    "                    for i in range(0, n_samples*sample_stride, sample_stride)]\n",
    "    return fpathseqlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7245711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_root='/N/project/infant_image_statistics/preproc_saber/JPG_10fps/'\n",
    "ds_rate = 1\n",
    "\n",
    "n_groupframes = 1450000 # minimum number of frames across age groups\n",
    "\n",
    "g0='008MS+009SS_withrotation+010BF_withrotation+011EA_withrotation+012TT_withrotation+013LS+014SN+015JM+016TF+017EW_withrotation'\n",
    "g1='026AR+027SS+028CK+028MR+029TT+030FD+031HW+032SR+033SE+034JC_withlighting'\n",
    "g2='043MP+044ET+046TE+047MS+048KG+049JC+050AB+050AK_rotation+051DW'\n",
    "# Total number of frames in each age group: g0=1.68m, g1=1.77m, g2=1.45m\n",
    "\n",
    "g0 = g0.split('+')\n",
    "g1 = g1.split('+')\n",
    "g2 = g2.split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1060fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "num_frames = 16\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "image_processor = transformers.VideoMAEImageProcessor(\n",
    "    size={\"shortest_edge\":image_size},\n",
    "    do_center_crop=True, crop_size={\"height\":image_size, \"width\": image_size}\n",
    ")\n",
    "\n",
    "seq_len = num_frames #equivalent to num_frames in VideoMAE()\n",
    "#     ds_rate = 1\n",
    "n_samples = None#10 #50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecf2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(subj_dirs, **kwargs):\n",
    "    seq_len = kwargs['seq_len']\n",
    "    n_groupframes=kwargs['n_groupframes']#1450000\n",
    "    ds_rate = kwargs['ds_rate']\n",
    "    jpg_root = kwargs['jpg_root']\n",
    "    \n",
    "    gx_fpathlist = []\n",
    "    for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "        gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)\n",
    "    gx_fpathlist = gx_fpathlist[:n_groupframes]\n",
    "\n",
    "    # Train-val split\n",
    "    gx_train_fp, gx_val_fp = get_train_val_split(gx_fpathlist, val_ratio=0.1)\n",
    "\n",
    "\n",
    "    gx_train_fpathseqlist = get_fpathseqlist(gx_train_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    gx_val_fpathseqlist = get_fpathseqlist(gx_val_fp, seq_len, ds_rate=1, n_samples=None)\n",
    "    \n",
    "    return {'train':ImageSequenceDataset(gx_train_fpathseqlist, transform=image_processor),\n",
    "           'val': ImageSequenceDataset(gx_val_fpathseqlist, transform=image_processor)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645acb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 9/9 [00:10<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets = make_dataset(g2, seq_len=seq_len, jpg_root=jpg_root, ds_rate=ds_rate, n_groupframes=n_groupframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0165aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "        datasets[x], batch_size=batch_size)\n",
    "                        for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb954915",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 768 #384\n",
    "intermediate_size = 3072 #4*384\n",
    "num_atention_heads = 12 #6\n",
    "\n",
    "config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                     num_frames=num_frames, tubelet_size=2, \n",
    "                                     hidden_size=hidden_size, num_hidden_layers=12, num_attention_heads=num_atention_heads,\n",
    "                                     intermediate_size=intermediate_size, initializer_range=0.02,\n",
    "                                     use_mean_pooling=True, decoder_num_attention_heads=6,\n",
    "                                     decoder_hidden_size=384, decoder_num_hidden_layers=4, \n",
    "                                     decoder_intermediate_size=1536, norm_pix_loss=True)\n",
    "# config\n",
    "model = transformers.VideoMAEForPreTraining(config)\n",
    "#model components: base_model, encoder_to_decoder, decoder\n",
    "# model.videomae==model.base_model\n",
    "# model.videomae.embeddings, model.videomae.encoder\n",
    "\n",
    "# embeddings: a Conv3D layer: Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
    "# Splits the image sequence into n tubes \n",
    "# Maps each tube (2x16x16) to a 768D vector using linear projection (one projector for all tubes)\n",
    "# Returns a tensor of shape (789x768) for each image sequence.\n",
    " \n",
    "# encoder: has 12 layers of type VideoMAELayer. Each layer is: [attention, linear, linear(768,3073), gelu, linear(3072,368), layernorm]\n",
    "\n",
    "# encoder_to_decoder: one linear layer: (in_features=768, out_features=384, bias=False)\n",
    "\n",
    "# decoder: has 4 VideoMAELayer layers + a linear projection from 384 to 1536 dimensions\n",
    "\n",
    "\n",
    "# If you use the VideoMAEModel, it only includes the base model (encoder). \n",
    "# You may or may not choose to pass in bool_masked_pos argument.\n",
    "# If you don't, the output is of shape: [1568, 768] (the eoncoding of all tubes?)\n",
    "# If you do, the output is of a different shape: [789, 768] (the predictions for the masked tubes?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c3e79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                       | 0/81562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 0/81562 [00:02<?, ?it/s]\n",
      "  0%|                                        | 0/9062 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 0/9062 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "model_seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n",
    "    \n",
    "    \n",
    "for phase in ['train', 'val']:\n",
    "#     dataloaders_dict[phase].sampler.set_epoch(i_ep)\n",
    "    if phase == 'train':\n",
    "        model.train()  # Set model to training mode\n",
    "    else:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs in tqdm(dataloaders[phase]):\n",
    "        print(inputs.shape)\n",
    "#         break\n",
    "        optimizer.zero_grad()\n",
    "        bool_masked_pos = torch.randint(0, 2, (batch_size, model_seq_length)).bool()\n",
    "        outputs = model(inputs, bool_masked_pos=bool_masked_pos)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
