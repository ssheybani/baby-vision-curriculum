{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2e8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Report downstream performance scores for a pretrained model.\n",
    "\"\"\"\n",
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '38' #@@@@ to help with the num_workers issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  #10\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from ddputils import is_main_process, save_on_master, setup_for_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7242c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5557f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "# Get Model\n",
    "def get_config(image_size, args, num_labels=2):\n",
    "    arch_kw = args.architecture\n",
    "    \n",
    "    if arch_kw=='base': #default\n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=args.num_frames, tubelet_size=args.tubelet_size, \n",
    "                                             hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n",
    "                                             intermediate_size=3072, num_labels=num_labels)\n",
    "    elif arch_kw=='small2':\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 4*768\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=args.num_frames, tubelet_size=args.tubelet_size, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, \n",
    "                                             num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "    \n",
    "    elif arch_kw=='small1':\n",
    "        hidden_size = 384\n",
    "        intermediate_size = 4*384\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 12\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=args.num_frames, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "        \n",
    "    elif arch_kw=='small3':\n",
    "        hidden_size = 384\n",
    "        intermediate_size = 4*384\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError\n",
    "    return config\n",
    "\n",
    "\n",
    "def init_model_from_checkpoint(model, checkpoint_path):\n",
    "    # caution: model class\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "def adapt_videomae(source_model, target_model):\n",
    "    # load the embeddings\n",
    "    target_model.videomae.embeddings.load_state_dict(\n",
    "        source_model.videomae.embeddings.state_dict())\n",
    "#     load the encoder\n",
    "    target_model.videomae.encoder.load_state_dict(\n",
    "        source_model.videomae.encoder.state_dict())\n",
    "    return target_model\n",
    "# def adapt_videomae(source_model, target_model):\n",
    "#     # load the embeddings\n",
    "#     target_model.embeddings.load_state_dict(\n",
    "#         source_model.videomae.embeddings.state_dict())\n",
    "# #     load the encoder\n",
    "#     target_model.encoder.load_state_dict(\n",
    "#         source_model.videomae.encoder.state_dict())\n",
    "#     return target_model\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "#         for param in model.classifier.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "def get_model(image_size, num_labels, feature_extracting, args):\n",
    "    config_source = get_config(image_size, args)\n",
    "    model_source = transformers.VideoMAEForPreTraining(config_source)\n",
    "    \n",
    "    if args.init_checkpoint_path!='na':\n",
    "        print('args.init_checkpoint_path:',args.init_checkpoint_path)\n",
    "        # initialize the model using the checkpoint\n",
    "        model_source = init_model_from_checkpoint(model_source, args.init_checkpoint_path)\n",
    "  \n",
    "    config_target = get_config(image_size, args, num_labels=num_labels)\n",
    "    model_target = transformers.VideoMAEForVideoClassification(config=config_target)\n",
    "#     model_target = transformers.VideoMAEModel(config=config_target) #@@@ do not add the classifer head\n",
    "    model_target = adapt_videomae(model_source, model_target)\n",
    "#     if not torch.all(\n",
    "#         model_target.embeddings.patch_embeddings.projection.weight==model_source.videomae.embeddings.patch_embeddings.projection.weight):\n",
    "#         warnings.warn('Model not successfully initialized')\n",
    "    if not torch.all(\n",
    "        model_target.videomae.embeddings.patch_embeddings.projection.weight==model_source.videomae.embeddings.patch_embeddings.projection.weight):\n",
    "        warnings.warn('Model not successfully initialized')\n",
    "    \n",
    "#     if feature_extracting: #@@@@@ redundant with torch.no_grad\n",
    "#         set_parameter_requires_grad(model_target, feature_extracting)\n",
    "    \n",
    "    return model_target\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "\n",
    "def save_results(fnames, embeddings, args):\n",
    "    print('embeddings.shape:',embeddings.shape)\n",
    "    print('len(fnames):',len(fnames))\n",
    "#     print('type(fnames):',type(fnames))\n",
    "    try:\n",
    "        print('type(fnames[0]):',type(fnames[0]))\n",
    "    except:\n",
    "        pass\n",
    "#     if type(fnames[0])==list:\n",
    "#         print('len(fnames[0]):',len(fnames[0]))\n",
    "    hdim = embeddings.shape[1]\n",
    "    xdf = pd.DataFrame(embeddings, columns= ['dim'+str(i)\n",
    "                                         for i in range(hdim)])\n",
    "    xdf['fnames'] = fnames\n",
    "    xdf = xdf[['fnames']+ list(xdf.columns[:-1])]\n",
    "\n",
    "    xdf = xdf.sort_values('fnames')\n",
    "    xdf = xdf.drop_duplicates(subset='fnames', ignore_index=True)\n",
    "\n",
    "    savedir = args.savedir\n",
    "    Path(savedir).mkdir(parents=True, exist_ok=True)\n",
    "#         <model vs scores>_<prot>_seed_<seed>_other_<other>_<other id>\n",
    "    result_fname = '_'.join(['embeddings', args.prot_name, \n",
    "                            'seed', str(args.seed),  \n",
    "                            args.other_id])+'.csv'\n",
    "    results_fpath = os.path.join(savedir, result_fname)\n",
    "    xdf.to_csv(results_fpath, sep=',', float_format='%.6f', index=False)\n",
    "    print('embeddings saved at ',results_fpath)\n",
    "    \n",
    "    \n",
    "def setup(rank, world_size):    \n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'    \n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8235d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "\n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)\n",
    "\n",
    "def transform_vid(video):\n",
    "    # Used with standard video datasets such as torchvision.UCF101\n",
    "#     print(vid.shape)\n",
    "    if video.shape[1]!=3: # Make it TCHW\n",
    "        video = torch.permute(video, (0,3,1,2))\n",
    "    image_size = 224\n",
    "#     vid.p\n",
    "    transform = _get_transform(image_size)\n",
    "#     xtt = [transform(torch.from_numpy(frame)).unsqueeze(0) \n",
    "    xtt = [transform(frame).unsqueeze(0) \n",
    "       for frame in video]\n",
    "    return torch.concat(xtt, axis=0)#.unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "class SSv2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform, frame_rate=12, sample_len=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.frame_rate = frame_rate\n",
    "        self.sample_len = sample_len\n",
    "        self.transform = transform\n",
    "        self.samples = sorted(os.listdir(root_dir),\n",
    "                             key=lambda x: int(x))\n",
    "        self.fps = 12 #original frame rate\n",
    "        self.ds_rate = round(self.fps/self.frame_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _read_frames(self, sample_dir, framefn_list):\n",
    "        vid= [torchvision.io.read_image(\n",
    "            str(Path(self.root_dir, sample_dir, fn))).unsqueeze(0)\n",
    "                for fn in framefn_list]\n",
    "        return torch.concat(vid, axis=0)\n",
    "        \n",
    "    def get_frames(self, sample_dir):\n",
    "        framefn_list = sorted(os.listdir(self.root_dir+sample_dir),\n",
    "                              key=lambda x: int(x.split('.')[0]))\n",
    "        # try selecting with the suggested ds_rate, starting from the suggested point.\n",
    "        # if not enough, try starting from the beginning\n",
    "        # if still not enough, gradually reduce ds_rate.\n",
    "        # if not enough frames for ds_rate=1, repeat the last frame.\n",
    "        num_frames = len(framefn_list)\n",
    "        loc_idx = num_frames//4\n",
    "        slen = self.sample_len\n",
    "        step = self.ds_rate\n",
    "        if num_frames//step <slen:\n",
    "            last_item = framefn_list[-1]\n",
    "            while (len(framefn_list)//step)<slen:\n",
    "                framefp_list.append(last_item)\n",
    "            return self._read_frames(sample_dir, framefn_list[::step][:slen])\n",
    "        elif (num_frames-loc_idx)//step <slen:\n",
    "            return self._read_frames(sample_dir, framefn_list[::step][:slen])\n",
    "        else:\n",
    "            return self._read_frames(sample_dir, framefn_list[loc_idx:loc_idx+slen*step:step][:slen])\n",
    "\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         print('---------------')\n",
    "        vid_fname = self.samples[index]\n",
    "        frames = self.get_frames(vid_fname)\n",
    "#         if self.transform is not None:\n",
    "#             frames = [self.transform(fr)\n",
    "#                       for fr in frames]\n",
    "        if self.transform is not None:\n",
    "            frames = self.transform(frames)\n",
    "        return frames, vid_fname\n",
    "        \n",
    "            \n",
    "def my_collate(batch):\n",
    "    batch = tuple(filter(lambda x: x[0] is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a6a9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e45f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transform_vid\n",
    "ssv2_root = '/N/project/baby_vision_curriculum/benchmarks/ssv2/easy_frames/'\n",
    "dset = SSv2Dataset(ssv2_root, transform, frame_rate=12, sample_len=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32490927",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt, ytt = dset[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0310997c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'512'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25f37cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21da176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_root = '/N/project/baby_vision_curriculum/benchmarks/ssv2/20bn-something-something-v2/'\n",
    "easy_annot_path = '/N/project/baby_vision_curriculum/benchmarks/ssv2/easy_labels/train_ann_b-10classes.json'\n",
    "frame_rate = 15\n",
    "num_frames=16\n",
    "args = Args(vid_root=vid_root, \n",
    "          frame_rate=frame_rate,\n",
    "          num_frames=num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36769b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpathlist = sorted([fp for fp in os.listdir(args.vid_root)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409d20d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198761.webm</td>\n",
       "      <td>Putting [something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33526.webm</td>\n",
       "      <td>Moving [something] from left to right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59835.webm</td>\n",
       "      <td>Moving [something] from left to right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27220.webm</td>\n",
       "      <td>Picking [something] up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75898.webm</td>\n",
       "      <td>Moving [something] from right to left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>128959.webm</td>\n",
       "      <td>Pouring [something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>142511.webm</td>\n",
       "      <td>Moving [something] from right to left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>149280.webm</td>\n",
       "      <td>Putting [something]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>156722.webm</td>\n",
       "      <td>Moving [something] from left to right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>138959.webm</td>\n",
       "      <td>Moving [something] from right to left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            fname                                  label\n",
       "0     198761.webm                    Putting [something]\n",
       "1      33526.webm  Moving [something] from left to right\n",
       "2      59835.webm  Moving [something] from left to right\n",
       "3      27220.webm                 Picking [something] up\n",
       "4      75898.webm  Moving [something] from right to left\n",
       "...           ...                                    ...\n",
       "9995  128959.webm                    Pouring [something]\n",
       "9996  142511.webm  Moving [something] from right to left\n",
       "9997  149280.webm                    Putting [something]\n",
       "9998  156722.webm  Moving [something] from left to right\n",
       "9999  138959.webm  Moving [something] from right to left\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_csv_file = '/N/project/baby_vision_curriculum/benchmarks/ssv2/easy_labels/train_easy10.csv'\n",
    "df = pd.read_csv(output_csv_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff51d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(fp)\n",
    "if cap is None or not cap.isOpened():\n",
    "    warnings.warn('unable to open video source: '+vid_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea0561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae94c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(fp, save_dir):\n",
    "    cap = cv2.VideoCapture(fp)\n",
    "    i=0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        save_path = os.path.join(save_dir,\n",
    "                         str(i)+'.jpg')\n",
    "        print(save_path)\n",
    "#         cv2.imwrite(save_path, frame)\n",
    "        i+=1\n",
    "    cap.release()\n",
    "    return\n",
    "\n",
    "\n",
    "# def extract_frames(fp, save_dir, ds_rate=2):\n",
    "    \n",
    "#     frames = _collect_all_frames(fp)\n",
    "    \n",
    "#     n_frames = len(frames)\n",
    "#     if n_frames==0:\n",
    "#         warnings.warn(fp+' has no frames')\n",
    "#     if n_frames/ds_rate\n",
    "# #         if i%ds_rate==0:\n",
    "# #             cv2.imwrite(save_dir+str(i)+'.jpg',frame)\n",
    "#         i+=1\n",
    "#     cap.release()\n",
    "#     return i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0655a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = args.vid_root+df['fname'].iloc[20]\n",
    "# output_jpg_dir = '/N/project/baby_vision_curriculum/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7b2f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames(fp, output_jpg_dir, ds_rate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b84422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:37<00:00, 13.33it/s]\n"
     ]
    }
   ],
   "source": [
    "n_frames = []\n",
    "for fn in tqdm(df['fname'].iloc[::20]):\n",
    "    fp = args.vid_root+fn\n",
    "    n_frames.append(\n",
    "        extract_frames(fp, output_jpg_dir, ds_rate=1))\n",
    "#     print(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5a81fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  2.,  2.,  0.,  2.,  0.,  3.,  1.,  4.,  1.,  3.,  6., 17.,\n",
       "        11.,  9.,  7., 21., 15., 12., 14., 28., 22.,  9., 18., 10., 33.,\n",
       "        14., 11., 16., 25.,  9., 10., 17., 28., 10.,  5.,  7., 14.,  7.,\n",
       "         6., 11., 17., 11.,  5.,  3., 13.,  1.,  3.,  3.,  3.]),\n",
       " array([11.  , 12.24, 13.48, 14.72, 15.96, 17.2 , 18.44, 19.68, 20.92,\n",
       "        22.16, 23.4 , 24.64, 25.88, 27.12, 28.36, 29.6 , 30.84, 32.08,\n",
       "        33.32, 34.56, 35.8 , 37.04, 38.28, 39.52, 40.76, 42.  , 43.24,\n",
       "        44.48, 45.72, 46.96, 48.2 , 49.44, 50.68, 51.92, 53.16, 54.4 ,\n",
       "        55.64, 56.88, 58.12, 59.36, 60.6 , 61.84, 63.08, 64.32, 65.56,\n",
       "        66.8 , 68.04, 69.28, 70.52, 71.76, 73.  ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(n_frames, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09743e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo code\n",
    "# load only the 10k easy videos. (Consider extracting the frames of those 10k videos and placing them in \n",
    "# ssv2/easy10)\n",
    "\n",
    "# there are 10-70 frames in each video.\n",
    "# for videomae, ds_rate=2 might be good. starting from the middle\n",
    "# 1- it's somewhat important to cover most of the duration\n",
    "# 2- it's also important to cover temporal precision. \n",
    "# 2 is more important\n",
    "\n",
    "# collect all_frames\n",
    "# try selecting with the suggested ds_rate, starting from the suggested point.\n",
    "# if not enough, try starting from the beginning\n",
    "# if still not enough, gradually reduce ds_rate.\n",
    "# if not enough frames for ds_rate=1, repeat the last frame.\n",
    "\n",
    "# set ds_rate to 2.\n",
    "# set the starting point to 1/4th frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "toybox_root = args.vid_root\n",
    "#     '/N/project/baby_vision_curriculum/benchmarks/toybox/vids/toybox/'\n",
    "transform = transform_vid\n",
    "frame_rate=args.frame_rate\n",
    "sample_len=args.num_frames\n",
    "dataset = ToyboxDataset(toybox_root, transform, \n",
    "                      frame_rate=frame_rate, sample_len=sample_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
