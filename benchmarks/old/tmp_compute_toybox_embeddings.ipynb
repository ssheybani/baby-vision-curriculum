{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7b40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '38' #@@@@ to help with the num_workers issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  #10\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "from ddputils import is_main_process, save_on_master, setup_for_distributed\n",
    "\n",
    "# torchvision.disable_beta_transforms_warning()\n",
    "# import torchvision.transforms.v2 as tr #May 9: would require reinstalling th evirtual env.\n",
    "# we might do it later.\n",
    "\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe()))) #os.getcwd() #\n",
    "# # print('cwd: ',SCRIPT_DIR)\n",
    "# #os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe())))\n",
    "# util_path = os.path.normpath(os.path.join(SCRIPT_DIR, '..', 'util'))\n",
    "# sys.path.insert(0, util_path)    \n",
    "\n",
    "\n",
    "# from train_downstream_VideoMAE import train_classifier_ddp\n",
    "# from make_toybox_dataset import make_toybox_dataset\n",
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from torch.utils.data import Dataset\n",
    "import av\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0563f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "\n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)\n",
    "\n",
    "def transform_vid(video):\n",
    "    # Used with standard video datasets such as torchvision.UCF101\n",
    "#     print(vid.shape)\n",
    "    if video.shape[1]!=3: # Make it TCHW\n",
    "        video = torch.permute(video, (0,3,1,2))\n",
    "    image_size = 224\n",
    "#     vid.p\n",
    "    transform = _get_transform(image_size)\n",
    "#     xtt = [transform(torch.from_numpy(frame)).unsqueeze(0) \n",
    "    xtt = [transform(frame).unsqueeze(0) \n",
    "       for frame in video]\n",
    "    return torch.concat(xtt, axis=0)#.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad6543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(image_size, args, num_labels=2):\n",
    "    arch_kw = args.architecture\n",
    "    if arch_kw=='small2':\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 4*768\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "    \n",
    "    elif len(arch_kw)==0: #default\n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n",
    "                                             intermediate_size=3072, num_labels=num_labels)\n",
    "    elif arch_kw=='small1':\n",
    "        hidden_size = 384\n",
    "        intermediate_size = 4*384\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 12\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError\n",
    "    return config\n",
    "\n",
    "\n",
    "def init_model_from_checkpoint(model, checkpoint_path):\n",
    "    # caution: model class\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "def adapt_videomae(source_model, target_model):\n",
    "    # load the embeddings\n",
    "    target_model.videomae.embeddings.load_state_dict(\n",
    "        source_model.videomae.embeddings.state_dict())\n",
    "#     load the encoder\n",
    "    target_model.videomae.encoder.load_state_dict(\n",
    "        source_model.videomae.encoder.state_dict())\n",
    "    return target_model\n",
    "# def adapt_videomae(source_model, target_model):\n",
    "#     # load the embeddings\n",
    "#     target_model.embeddings.load_state_dict(\n",
    "#         source_model.videomae.embeddings.state_dict())\n",
    "# #     load the encoder\n",
    "#     target_model.encoder.load_state_dict(\n",
    "#         source_model.videomae.encoder.state_dict())\n",
    "#     return target_model\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "#         for param in model.classifier.parameters():\n",
    "#             param.requires_grad = True\n",
    "            \n",
    "def get_model(image_size, num_labels, feature_extracting, args):\n",
    "    config_source = get_config(image_size, args)\n",
    "    model_source = transformers.VideoMAEForPreTraining(config_source)\n",
    "    \n",
    "    if args.init_checkpoint_path!='na':\n",
    "        print('args.init_checkpoint_path:',args.init_checkpoint_path)\n",
    "        # initialize the model using the checkpoint\n",
    "        model_source = init_model_from_checkpoint(model_source, args.init_checkpoint_path)\n",
    "  \n",
    "    config_target = get_config(image_size, args, num_labels=num_labels)\n",
    "    model_target = transformers.VideoMAEForVideoClassification(config=config_target)\n",
    "#     model_target = transformers.VideoMAEModel(config=config_target) #@@@ do not add the classifer head\n",
    "    model_target = adapt_videomae(model_source, model_target)\n",
    "#     if not torch.all(\n",
    "#         model_target.embeddings.patch_embeddings.projection.weight==model_source.videomae.embeddings.patch_embeddings.projection.weight):\n",
    "#         warnings.warn('Model not successfully initialized')\n",
    "    if not torch.all(\n",
    "        model_target.videomae.embeddings.patch_embeddings.projection.weight==model_source.videomae.embeddings.patch_embeddings.projection.weight):\n",
    "        warnings.warn('Model not successfully initialized')\n",
    "    \n",
    "    if feature_extracting:\n",
    "        set_parameter_requires_grad(model_target, feature_extracting)\n",
    "    \n",
    "    return model_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7af384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df176e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_name='g0g1'\n",
    "seed=401\n",
    "\n",
    "rank = 'cuda:0'\n",
    "image_size= 224\n",
    "num_classes=0# to get only the features, no classifier head\n",
    "feature_extract=True\n",
    "architecture='small2'\n",
    "init_checkpoint_path='na'\n",
    "savedir='/N/project/baby_vision_curriculum/trained_models/generative/v2/benchmarks/toybox/'\n",
    "batch_size=128#64\n",
    "other_id='10fps.3ep'\n",
    "frame_rate=10\n",
    "num_workers=6\n",
    "\n",
    "args = Args(architecture=architecture,\n",
    "            init_checkpoint_path=init_checkpoint_path,\n",
    "            savedir=savedir,\n",
    "            prot_name=prot_name,\n",
    "            seed=seed,\n",
    "            other_id=other_id,\n",
    "            frame_rate=frame_rate,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0b04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodel = get_model(image_size, num_classes, feature_extract, args)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9331e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "# inputs['pixel_values'].shape\n",
    "# outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93d7c0",
   "metadata": {},
   "source": [
    "## Prepare the toybox dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c1eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "class ToyboxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform, frame_rate=10, sample_len=16):\n",
    "        self.root_dir = root_dir\n",
    "        self.frame_rate = frame_rate\n",
    "        self.sample_len = sample_len\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for supercategory in os.listdir(self.root_dir):\n",
    "            for obj in os.listdir(os.path.join(self.root_dir, supercategory)):\n",
    "#                 for obj in os.listdir(os.path.join(self.root_dir, supercategory, category)):\n",
    "                object_dir = os.path.join(self.root_dir, supercategory, obj)\n",
    "                for view in os.listdir(object_dir):\n",
    "                    view_path = os.path.join(object_dir, view)\n",
    "                    self.samples.append(view_path)\n",
    "#                         self.samples.append((view_path, supercategory, category, object))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_all_frames(self, cap):\n",
    "        desired_frames = self.sample_len\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "#                 print('end of the video, i_frame, len fames', frame_count, len(frames))\n",
    "                # End of video\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)                \n",
    "            if len(frames) == desired_frames:\n",
    "                break\n",
    "        tmp_nframes = len(frames)\n",
    "        if tmp_nframes < desired_frames:\n",
    "            last_frame = frames[-1]\n",
    "            for i in range(desired_frames - tmp_nframes):\n",
    "                frames.append(last_frame)\n",
    "        \n",
    "        assert len(frames)==desired_frames\n",
    "        return frames\n",
    "    \n",
    "    def wrap_frames(self, frames):\n",
    "        frames = torch.as_tensor(np.asarray(frames))\n",
    "        if len(frames.shape)!=4: #torch.Size([16, 12xx, 19xx, 3])\n",
    "            return None\n",
    "        return self.transform(frames)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "#         print('---------------')\n",
    "        vid_path = self.samples[index]\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "        if cap is None or not cap.isOpened():\n",
    "            warnings.warn('unable to open video source: '+vid_path)\n",
    "            return None, None\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        ds_rate = round(fps/self.frame_rate)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         print('num_frames:',num_frames)\n",
    "#         print('ds_rate:',ds_rate)\n",
    "#         print('num_frames:',num_frames)\n",
    "        \n",
    "        sample_scope = self.sample_len*ds_rate\n",
    "        if num_frames<sample_scope:\n",
    "#             print('Not enough frames in the video',vid_path)\n",
    "            frames = self.get_all_frames(cap)\n",
    "                        #apply transform\n",
    "            frames_transformed = self.wrap_frames(frames)\n",
    "            if frames_transformed is None:\n",
    "                print(vid_path, 'gave None')\n",
    "                return None, None\n",
    "            return frames_transformed, vid_path\n",
    "            \n",
    "        \n",
    "        # duration = num_frames / fps\n",
    "        start_frame = int(num_frames * 1 / 5)  # Starting frame at 2/3 of video duration\n",
    "        if (num_frames-start_frame)<sample_scope:\n",
    "            start_frame = num_frames-sample_scope\n",
    "        \n",
    "#         print('start_frame',start_frame)\n",
    "#         end_frame = start_frame+sample_scope#int(start_frame + fps * 1.6)  # Ending frame after 1.6 seconds\n",
    "        desired_frames = self.sample_len\n",
    "        frame_count = 0\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "#                 print('end of the video, i_frame, len fames', frame_count, len(frames))\n",
    "                # End of video\n",
    "                break\n",
    "            \n",
    "            if frame_count % ds_rate==0:\n",
    "#                 if (frame_count > start_frame) & \\\n",
    "#                 (frame_count < end_frame):\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)                \n",
    "            if len(frames) == desired_frames:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        frames_transformed = self.wrap_frames(frames)\n",
    "        \n",
    "        if frames_transformed is None:\n",
    "            print(vid_path, 'gave None')\n",
    "            return None, None\n",
    "        else:\n",
    "            return frames_transformed, vid_path\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbfde00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toybox_root = '/N/project/baby_vision_curriculum/benchmarks/toybox/vids/toybox/'\n",
    "transform = transform_vid\n",
    "frame_rate=3\n",
    "sample_len=16\n",
    "tb_dataset = ToyboxDataset(toybox_root, transform, \n",
    "                          frame_rate=frame_rate, sample_len=sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff7ec6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape==torch.Size([16, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2410cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt = []\n",
    "for i in range(2000,3000,100):\n",
    "    t0 = time()\n",
    "    sx = tb_dataset[i][0]\n",
    "    xtt.append(deepcopy(sx))\n",
    "    print(time()-t0)\n",
    "    assert len(sx)==16\n",
    "# takes 1 second. might get faster later because of cache\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,8, figsize=(10,3))\n",
    "j=9\n",
    "for i in range(8):\n",
    "    ax[i].imshow(xtt[j][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9532d98",
   "metadata": {},
   "source": [
    "## Inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f35ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch = tuple(filter(lambda x: x[0] is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fa4fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_shuffle = True #for the distributed dampler\n",
    "# num_epochs = args.n_epoch\n",
    "batch_size = args.batch_size# 128\n",
    "pin_memory = False\n",
    "num_workers = args.num_workers #number_of_cpu-1#32\n",
    "collate_fn = my_collate#None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff149751",
   "metadata": {},
   "outputs": [],
   "source": [
    "toybox_root = '/N/project/baby_vision_curriculum/benchmarks/toybox/vids/toybox/'\n",
    "transform = transform_vid\n",
    "frame_rate=3\n",
    "sample_len=16\n",
    "dataset = ToyboxDataset(toybox_root, transform, \n",
    "                          frame_rate=frame_rate, sample_len=sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b6f70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, pin_memory=pin_memory, collate_fn=collate_fn,\n",
    "        num_workers=num_workers, shuffle=False, drop_last=False)#, sampler=samplers_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6960d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xmodel = xmodel.to(rank)\n",
    "_ = xmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd38980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(fnames, embeddings, args):\n",
    "    hdim = embeddings.shape[1]\n",
    "    xdf = pd.DataFrame(embeddings, columns= ['dim'+str(i)\n",
    "                                         for i in range(hdim)])\n",
    "    xdf['fnames'] = fnames\n",
    "    xdf = xdf[['fnames']+ list(xdf.columns[:-1])]\n",
    "\n",
    "    xdf = xdf.sort_values('fnames')\n",
    "    xdf = xdf.drop_duplicates(subset='fnames', ignore_index=True)\n",
    "\n",
    "    savedir = args.savedir\n",
    "    Path(savedir).mkdir(parents=True, exist_ok=True)\n",
    "#         <model vs scores>_<prot>_seed_<seed>_other_<other>_<other id>\n",
    "    result_fname = '_'.join(['embeddings', args.prot_name, \n",
    "                            'seed', str(args.seed),  \n",
    "                            args.other_id])+'.csv'\n",
    "    results_fpath = os.path.join(savedir, result_fname)\n",
    "    results_df.to_csv(results_fpath, sep=',', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc19991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▏                                       | 1/34 [02:13<1:13:28, 133.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                         | 2/34 [02:15<29:54, 56.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▉                                        | 3/34 [02:17<16:10, 31.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▏                                      | 4/34 [02:19<09:51, 19.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▍                                     | 5/34 [02:21<06:24, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▊                                    | 6/34 [02:22<04:22,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_81364/327569707.py:58: UserWarning: unable to open video source: /N/project/baby_vision_curriculum/benchmarks/toybox/vids/toybox/vehicles/airplane_01_pivothead/._airplane_01_pivothead_absent.mp4\n",
      "  warnings.warn('unable to open video source: '+vid_path)\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x163c90c0] moov atom not found\n",
      " 21%|█████████                                   | 7/34 [03:55<16:26, 36.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████▎                                 | 8/34 [03:57<11:02, 25.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████▋                                | 9/34 [03:59<07:33, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▋                              | 10/34 [04:00<05:14, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 1343.09 MB\n",
      "GPU memory allocated: 1343.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▋                              | 10/34 [04:32<10:54, 27.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# all_embeddings = []\n",
    "world_size = 4\n",
    "data = {\n",
    "        'fnames':[],\n",
    "        'embeddings': []   \n",
    "    }\n",
    "outputs = [None for _ in range(world_size)]\n",
    "\n",
    "print_period=1#20\n",
    "i_break = 10\n",
    "with torch.no_grad():\n",
    "    for i_t, xbatch in enumerate(tqdm(dataloader)):\n",
    "        inputs, fnames = xbatch\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        inputs = inputs.to(rank)\n",
    "        image_features = xmodel(pixel_values=inputs).logits\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        data['fnames'] += fnames\n",
    "        data['embeddings'].append(image_features.detach().cpu().numpy())\n",
    "\n",
    "        if (i_t%print_period)==0:\n",
    "            memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "            print(f'GPU memory allocated: {memory_allocated:.2f} MB')\n",
    "        if i_t==i_break:\n",
    "            break #@@@\n",
    "\n",
    "# dist.all_gather_object(outputs, data)\n",
    "\n",
    "            \n",
    "#     if is_main_process():\n",
    "#         print('finished processing')\n",
    "#         allfnames, allembeddings = [],[]\n",
    "#         for cdict in outputs:\n",
    "#             allfnames += list(chain(*cdict['fnames'])) \n",
    "#             print('Aggregating worker results:',len(cdict['fnames']),'/', len(allfnames))\n",
    "#             allembeddings +=cdict['embeddings']\n",
    "            \n",
    "#         allembeddings = np.concatenate(allembeddings)\n",
    "    \n",
    "#         save_results(allfnames, allembeddings, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d1cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
