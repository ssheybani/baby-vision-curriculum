{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0329df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)\n",
    "\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '38' #@@@@ to help with the num_workers issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  #10\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torchvision import transforms as tr\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54a8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEConfig, VideoMAEModel\n",
    "from torch.utils.data import Dataset\n",
    "import av\n",
    "\n",
    "\n",
    "def get_config(image_size, args, num_labels=2):\n",
    "    arch_kw = args.architecture\n",
    "    if arch_kw=='small2':\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 4*768\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "    \n",
    "    elif len(arch_kw)==0: #default\n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=768, num_hidden_layers=12, num_attention_heads=12,\n",
    "                                             intermediate_size=3072, num_labels=num_labels)\n",
    "    elif arch_kw=='small1':\n",
    "        hidden_size = 384\n",
    "        intermediate_size = 4*384\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 12\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError\n",
    "    return config\n",
    "\n",
    "\n",
    "def init_model_from_checkpoint(model, checkpoint_path):\n",
    "    # caution: model class\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "def adapt_videomae(source_model, target_model):\n",
    "    # load the embeddings\n",
    "    target_model.videomae.embeddings.load_state_dict(\n",
    "        source_model.videomae.embeddings.state_dict())\n",
    "#     load the encoder\n",
    "    target_model.videomae.encoder.load_state_dict(\n",
    "        source_model.videomae.encoder.state_dict())\n",
    "    return target_model\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "def get_model(image_size, num_labels, feature_extracting, args):\n",
    "    config_source = get_config(image_size, args)\n",
    "    model_source = transformers.VideoMAEForPreTraining(config_source)\n",
    "    \n",
    "    if args.init_checkpoint_path!='na':\n",
    "        print('args.init_checkpoint_path:',args.init_checkpoint_path)\n",
    "        # initialize the model using the checkpoint\n",
    "        model_source = init_model_from_checkpoint(model_source, args.init_checkpoint_path)\n",
    "  \n",
    "    config_target = get_config(image_size, args, num_labels=num_labels)\n",
    "    model_target = transformers.VideoMAEForVideoClassification(config=config_target)\n",
    "    model_target = adapt_videomae(model_source, model_target)\n",
    "    if not torch.all(\n",
    "        model_target.videomae.embeddings.patch_embeddings.projection.weight==model_source.videomae.embeddings.patch_embeddings.projection.weight):\n",
    "        warnings.warn('Model not successfully initialized')\n",
    "    \n",
    "    set_parameter_requires_grad(model_target, feature_extracting)\n",
    "    \n",
    "    return model_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a02129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def get_optimizer(model, feature_extract, args):\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "#     if feature_extract:\n",
    "    lr = args.lr#1e-3\n",
    "    weight_decay =args.wd#5e-5\n",
    "    optimizer_ft = torch.optim.Adam(params_to_update, lr=lr, weight_decay=weight_decay)\n",
    "    #     optimizer_ft = torch.optim.SGD([{'params': params_to_update, \n",
    "    #                               'initial_lr':lr}], \n",
    "    #                             lr=lr, momentum=0.9)\n",
    "#     else:\n",
    "#         lr=1e-4\n",
    "#         optimizer_ft = torch.optim.Adam(params_to_update, lr=lr)\n",
    "        \n",
    "    return optimizer_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0b01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db153ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------\n",
    "# Dataset and Dataloader\n",
    "\n",
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "\n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)\n",
    "\n",
    "\n",
    "\n",
    "def transform_image_cifar10(image):\n",
    "#     Used for standard single image datasets such as torchvision.CIFAR10, torchvision.ImageNet\n",
    "#     if image.shape[0]!=3:\n",
    "    image_size=224\n",
    "    num_frames=16\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "\n",
    "    augs = [tr.ToTensor(),\n",
    "            tr.Resize(image_size), tr.CenterCrop(image_size),\n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    transform = tr.Compose(augs)\n",
    "\n",
    "    return transform(image).unsqueeze(0).repeat(num_frames,1,1,1)\n",
    "\n",
    "def get_inp_label(task, batch):\n",
    "    if task=='ucf101':\n",
    "        inputs, _, labels = batch\n",
    "        return inputs, labels\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def make_ucf101dataset(args):\n",
    "    ucf_root='/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF-101'\n",
    "    annotation_path = '/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/'\n",
    "    frames_per_clip = 16\n",
    "    step_between_clips = 1\n",
    "    frame_rate=args.frame_rate#int(30/args.ds_rate)\n",
    "    transform = transform_vid\n",
    "    output_format= 'TCHW'\n",
    "    num_workers=args.num_workers-1 #40\n",
    "    train_dataset = torchvision.datasets.UCF101(ucf_root, \n",
    "                                          annotation_path,\n",
    "                                          frames_per_clip,\n",
    "                                          step_between_clips=step_between_clips,\n",
    "                                          frame_rate=frame_rate,\n",
    "                                          fold=1,\n",
    "                                          train=True,\n",
    "                                          transform=transform,\n",
    "                                          output_format=output_format,\n",
    "                                          num_workers=num_workers)\n",
    "    val_dataset = torchvision.datasets.UCF101(ucf_root, \n",
    "                                          annotation_path,\n",
    "                                          frames_per_clip,\n",
    "                                          step_between_clips=step_between_clips,\n",
    "                                          frame_rate=frame_rate,\n",
    "                                          fold=1,\n",
    "                                          train=False,\n",
    "                                          transform=transform,\n",
    "                                          output_format=output_format,\n",
    "                                          num_workers=num_workers)\n",
    "    num_classes = 101\n",
    "    return {'train':train_dataset,\n",
    "           'val':val_dataset}, num_classes\n",
    "\n",
    "def make_cifar10dataset(args):\n",
    "    cifar10img_root = '/N/project/baby_vision_curriculum/benchmarks/mainstream/cifar10'\n",
    "    image_datasets = {'train': torchvision.datasets.CIFAR10(root=cifar10img_root,\n",
    "                                                            transform=transform_image_cifar10, train=True, download=True),\n",
    "                      'val': torchvision.datasets.CIFAR10(root=cifar10img_root,transform=transform_image_cifar10, train=False, download=True)}\n",
    "    num_classes = 10 \n",
    "        \n",
    "    return image_datasets, num_classes\n",
    "\n",
    "def make_dataset(args):\n",
    "    task = args.task\n",
    "    if task=='ucf101':\n",
    "        return make_ucf101dataset(args)\n",
    "#     seq_len = kwargs['seq_len']\n",
    "#     image_size = kwargs['image_size']\n",
    "    elif task=='cifar10':\n",
    "        return make_cifar10dataset(args)\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765a48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='cifar10'#'ucf101'\n",
    "ch_dir='/N/project/baby_vision_curriculum/trained_models/generative/v2/'\n",
    "\n",
    "init_checkpoint_path=ch_dir+\"model_g0_seed_1111_other_1111_mask50_small2_30ep.pt\"\n",
    "savedir='/N/project/baby_vision_curriculum/trained_models/generative/v2/benchmarks/ucf101/'\n",
    "prot_name='g0'\n",
    "seed=1111\n",
    "other_id='10fps.30ep'\n",
    "\n",
    "n_epoch=1\n",
    "save_model='n'\n",
    "\n",
    "frame_rate=10\n",
    "batch_size=64\n",
    "num_workers=45#6\n",
    "architecture='small2'\n",
    "lr=1e-3\n",
    "wd=5e-5\n",
    "args = Args(task=task, architecture=architecture,\n",
    "            init_checkpoint_path=init_checkpoint_path,\n",
    "            savedir=savedir,\n",
    "            prot_name=prot_name,\n",
    "            seed=seed,\n",
    "            other_id=other_id,\n",
    "            n_epoch=n_epoch,\n",
    "            save_model=save_model,\n",
    "            frame_rate=frame_rate,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr, \n",
    "           wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05bfb933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "datasets, num_classes = make_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15e3d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa6c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vid(video):\n",
    "    # Used with standard video datasets such as torchvision.UCF101\n",
    "#     print(vid.shape)\n",
    "    if video.shape[1]!=3: # Make it TCHW\n",
    "        video = torch.permute(video, (0,3,1,2))\n",
    "    image_size = 224\n",
    "#     vid.p\n",
    "    transform = _get_transform(image_size)\n",
    "#     xtt = [transform(torch.from_numpy(frame)).unsqueeze(0) \n",
    "    xtt = [transform(frame).unsqueeze(0) \n",
    "       for frame in video]\n",
    "    return torch.concat(xtt, axis=0)#.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ef46f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45114a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911e88ccf4194093b0fe3eb88c1d8732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torchvision/datasets/video_utils.py:223: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ucf_root='/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF-101'\n",
    "annotation_path = '/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/'\n",
    "frames_per_clip = 16\n",
    "step_between_clips = 1\n",
    "frame_rate=args.frame_rate#int(30/args.ds_rate)\n",
    "transform = transform_vid\n",
    "output_format= 'TCHW'\n",
    "num_workers=45# args.num_workers-1 #40\n",
    "train_dataset = torchvision.datasets.UCF101(ucf_root, \n",
    "                                      annotation_path,\n",
    "                                      frames_per_clip,\n",
    "                                      step_between_clips=step_between_clips,\n",
    "                                      frame_rate=frame_rate,\n",
    "                                      fold=1,\n",
    "                                      train=True,\n",
    "                                      transform=transform,\n",
    "                                      output_format=output_format,\n",
    "                                      num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9980da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/soft/sles15/deeplearning/Python-3.10.5/lib/python3.10/site-packages/torchvision/io/video.py:162: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(\n",
    "    datasets['train'][0][0]==train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a003c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "config_source = get_config(image_size, args)\n",
    "model_source = transformers.VideoMAEForVideoClassification(config_source)\n",
    "#transformers.VideoMAEForPreTraining(config_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6d0a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEForVideoClassification(\n",
       "  (videomae): VideoMAEModel(\n",
       "    (embeddings): VideoMAEEmbeddings(\n",
       "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): VideoMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): VideoMAELayer(\n",
       "          (attention): VideoMAEAttention(\n",
       "            (attention): VideoMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VideoMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VideoMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): VideoMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the dataset, criterion\n",
    "#     datasets, num_classes = make_dataset(args)\n",
    "feature_extract = True\n",
    "#     model_type = 'res50'\n",
    "#-----------------\n",
    "# Create the criterion\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Instantiate the model, optimizer\n",
    "#Load the model, adapt it to the downstream task\n",
    "image_size = 224\n",
    "feature_extract = True\n",
    "xmodel = get_model(image_size, num_classes, feature_extract, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f66795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a15f3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodel = xmodel.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2af61238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t classifier.weight\n",
      "\t classifier.bias\n"
     ]
    }
   ],
   "source": [
    "optimizer = get_optimizer(xmodel, feature_extract, args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa696548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucf_collate(batch):\n",
    "    filtered_batch = []\n",
    "    for video, _, label in batch:\n",
    "        filtered_batch.append((video, label))\n",
    "    return torch.utils.data.dataloader.default_collate(filtered_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a05ad8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_shuffle = True #for the distributed dampler\n",
    "num_epochs = args.n_epoch\n",
    "batch_size = args.batch_size# 128\n",
    "pin_memory = True\n",
    "num_workers = 5#args.num_workers #number_of_cpu-1#32\n",
    "if args.task=='ucf101':\n",
    "    collate_fn = ucf_collate\n",
    "else:\n",
    "    collate_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a495971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "        datasets[x], batch_size=batch_size, pin_memory=pin_memory, collate_fn=collate_fn,\n",
    "        num_workers=num_workers, shuffle=False, drop_last=True)\n",
    "                        for x in ['train', 'val']}\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=batch_size, pin_memory=False, \n",
    "#         num_workers=num_workers, shuffle=False, drop_last=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f66a9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dset, len dloader:  543459 33966\n",
      "dataloaders created\n"
     ]
    }
   ],
   "source": [
    "print('len dset, len dloader: ', len(datasets['train']), len(dataloaders['train']))\n",
    "#         print(dataset.__getitem__(22).shape)\n",
    "print('dataloaders created') #@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f47a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 2/33966 [00:03<15:34:03,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.321846008300781\n",
      "loss: 3.7920289039611816\n",
      "loss: 3.2920782566070557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                      | 2/33966 [00:05<25:27:39,  2.70s/it]\n",
      "  0%|                                       | 1/13213 [00:01<6:37:20,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.3286473751068115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                       | 2/13213 [00:02<3:10:21,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.337937116622925\n",
      "loss: 3.229137659072876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 2/13213 [00:03<5:58:05,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "rank='cuda:0'\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# best_model_wts = deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for phase in ['train', 'val']:\n",
    "#     dataloaders[phase].sampler.set_epoch(i_ep)\n",
    "    if phase == 'train':\n",
    "        xmodel.train()  # Set model to training mode\n",
    "    else:\n",
    "        xmodel.eval()   # Set model to evaluate mode\n",
    "\n",
    "    running_loss = torch.tensor([0.0], device='cuda:0')\n",
    "    running_corrects = torch.tensor([0.0], device=rank)\n",
    "\n",
    "    i_iter, print_period=0, 100\n",
    "    i_break, print_period = 3,1 #@@@ debug\n",
    "    # Iterate over data.\n",
    "    for batch in tqdm(dataloaders[phase]):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#                 loss, logits = get_loss(task, batch, phase, rank, args)\n",
    "\n",
    "        # implement get_loss for different datasets and for videomaeclassifier\n",
    "        inputs, labels = batch #get_inp_label(args.task, batch) \n",
    "        inputs = inputs.to(rank)\n",
    "        labels = labels.to(rank)\n",
    "        outputs = xmodel(pixel_values=inputs, labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "\n",
    "        _, preds = torch.max(logits, 1)\n",
    "\n",
    "        # backward + optimize only if in training phase\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "#                 print(rank, 'labels shape, device: ', labels.shape, labels.data.device)\n",
    "#                 print(rank, 'preds shape, device: ', preds.shape, preds.device)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        i_iter+=1\n",
    "        if (i_iter%print_period)==0:\n",
    "            print('loss:',loss.item())\n",
    "\n",
    "        if i_iter==i_break:\n",
    "            break #@@@@ debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f173558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 315.97 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "print(f'GPU memory allocated: {memory_allocated:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbb443ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150eb187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
