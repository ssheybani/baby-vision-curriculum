{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef46d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "env_root = '/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/'\n",
    "sys.path.insert(0, env_root)\n",
    "\n",
    "# SCRIPT_DIR = os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe()))) #os.getcwd() #\n",
    "# print('cwd: ',SCRIPT_DIR)\n",
    "#os.path.realpath(os.path.dirname(inspect.getfile(inspect.currentframe())))\n",
    "# util_path = os.path.normpath(os.path.join(SCRIPT_DIR, '..', 'util'))\n",
    "# sys.path.insert(0, util_path)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '38' #@@@@ to help with the num_workers issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'  #10\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "# from torchvision import transforms as tr\n",
    "# torchvision.disable_beta_transforms_warning()\n",
    "# import torchvision.transforms.v2 as tr\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "import os\n",
    "# import random\n",
    "# import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f67ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.0+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a3c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3fa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(image_size, args, num_labels=2):\n",
    "    arch_kw = args.architecture\n",
    "    if arch_kw=='small2':\n",
    "        hidden_size = 768\n",
    "        intermediate_size = 4*768\n",
    "        num_attention_heads = 6\n",
    "        num_hidden_layers = 6\n",
    "        \n",
    "        config = transformers.VideoMAEConfig(image_size=image_size, patch_size=16, num_channels=3,\n",
    "                                             num_frames=16, tubelet_size=2, \n",
    "                                             hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, num_attention_heads=num_attention_heads,\n",
    "                                             intermediate_size=intermediate_size, num_labels=num_labels)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return config\n",
    "\n",
    "def get_model(image_size, args):\n",
    "    config = get_config(image_size, args)\n",
    "    model = transformers.VideoMAEForPreTraining(config)\n",
    "    return model\n",
    "\n",
    "def init_model_from_checkpoint(model, checkpoint_path):\n",
    "    # caution: model class\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b58f081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.init_checkpoint_path: /N/project/baby_vision_curriculum/trained_models/generative/v2/s2/model_g1_seed_1133_other_1345_mask50_small2_pre.g0.pt\n"
     ]
    }
   ],
   "source": [
    "model_root = '/N/project/baby_vision_curriculum/trained_models/generative/v2/s2/'\n",
    "\n",
    "chpt_name = 'model_g1_seed_1133_other_1345_mask50_small2_pre.g0.pt'\n",
    "# 'model_g1_seed_1133_other_1345_mask50_small2_pre.g2.pt'\n",
    "\n",
    "chpt_path = model_root+chpt_name\n",
    "\n",
    "image_size = 224\n",
    "architecture='small2'\n",
    "init_checkpoint_path=chpt_path\n",
    "args = Args(architecture=architecture, init_checkpoint_path=init_checkpoint_path)\n",
    "\n",
    "pre_model = get_model(image_size, args)\n",
    "\n",
    "if args.init_checkpoint_path!='na':\n",
    "    print('args.init_checkpoint_path:',args.init_checkpoint_path)\n",
    "    # initialize the model using the checkpoint\n",
    "    pre_model = init_model_from_checkpoint(pre_model, args.init_checkpoint_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416e784",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0a3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_transform(image_size):\n",
    "\n",
    "    mean = [0.5, 0.5, 0.5]#np.mean(mean_all, axis=0) #mean_all[chosen_subj] \n",
    "    std = [0.25, 0.25, 0.25] #std_all[chosen_subj] \n",
    "    \n",
    "#     [0.485, 0.456, 0.406]  # IMAGENET_DEFAULT_MEAN\n",
    "#     [0.229, 0.224, 0.225]  # IMAGENET_DEFAULT_STD\n",
    "\n",
    "    augs = [tr.Resize(image_size), tr.CenterCrop(image_size), \n",
    "            tr.ConvertImageDtype(torch.float32), \n",
    "             tr.Normalize(mean,std)]\n",
    "    return tr.Compose(augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17766e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d221f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c215b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 16 frames\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a804424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video.shape\n",
    "video = np.moveaxis(video, -1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a067a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 3, 360, 640)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.moveaxis(video, -1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104fc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "transform = _get_transform(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f904cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt = [transform(torch.from_numpy(frame)).unsqueeze(0) \n",
    "       for frame in video]\n",
    "xtt = torch.concat(xtt, axis=0).unsqueeze(0)\n",
    "# torch.from_numpy(np.asarray([transform(frame) \n",
    "#                              for frame in video])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67b255a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aedd24e",
   "metadata": {},
   "source": [
    "## Initialize the classifier from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f2523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_videomae(source_model, target_model):\n",
    "    # load the embeddings\n",
    "    target_model.videomae.embeddings.load_state_dict(\n",
    "        source_model.videomae.embeddings.state_dict())\n",
    "#     load the encoder\n",
    "    target_model.videomae.encoder.load_state_dict(\n",
    "        source_model.videomae.encoder.state_dict())\n",
    "    return target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67f23469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 101\n",
    "config_ds = get_config(image_size, args, num_labels=num_labels)\n",
    "xmodel = VideoMAEForVideoClassification(config=config_ds)\n",
    "xmodel = adapt_videomae(pre_model, xmodel)\n",
    "torch.all(xmodel.videomae.embeddings.patch_embeddings.projection.weight==pre_model.videomae.embeddings.patch_embeddings.projection.weight)\n",
    "# del pre_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42660165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4779fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extracting = True\n",
    "set_parameter_requires_grad(xmodel, feature_extracting)\n",
    "## Sanity check\n",
    "# for param in xmodel.videomae.parameters(): #.classifier.parameters():#\n",
    "#     print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77ee1f",
   "metadata": {},
   "source": [
    "## Apply the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a56648ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor(([[22]]))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c9d6c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "labels = torch.tensor((22))\n",
    "outputs = xmodel(pixel_values=xtt, labels=labels)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "22595792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 101])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7381843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5aec6066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ea5e49e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc1803ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/project/baby_vision_curriculum/pythonenvs/hfenv/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:147: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "inputs = image_processor(list(video), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc850197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values\n"
     ]
    }
   ],
   "source": [
    "for item in inputs.keys():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85d49fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c07dd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a4ef",
   "metadata": {},
   "source": [
    "## Load UCF101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "893202b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vid(video):\n",
    "    # Used with standard video datasets such as torchvision.UCF101\n",
    "#     print(vid.shape)\n",
    "    if video.shape[1]!=3: # Make it TCHW\n",
    "        video = torch.permute(video, (0,3,1,2))\n",
    "    image_size = 224\n",
    "#     vid.p\n",
    "    transform = _get_transform(image_size)\n",
    "#     xtt = [transform(torch.from_numpy(frame)).unsqueeze(0) \n",
    "    xtt = [transform(frame).unsqueeze(0) \n",
    "       for frame in video]\n",
    "    return torch.concat(xtt, axis=0).unsqueeze(0)\n",
    "\n",
    "def transform_image(image):\n",
    "#     Used for standard single image datasets such as torchvision.CIFAR10, torchvision.ImageNet\n",
    "#     if image.shape[0]!=3:\n",
    "    image_size=224\n",
    "    num_frames=16\n",
    "    transform = _get_transform(image_size)\n",
    "    return transform(image).unsqueeze(0).repeat(num_frames,1,1,1)\n",
    "\n",
    "# xtt= torch.randint(0,255, (3, 640, 480))#torch.eye(3).unsqueeze(0)\n",
    "# ytt = transform_image(xtt)\n",
    "# ytt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e3ea18da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324d45abe32a48c0b2beb9f6f4b000e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/833 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ucf_root='/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF-101'\n",
    "annotation_path = '/N/project/baby_vision_curriculum/benchmarks/mainstream/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/'\n",
    "frames_per_clip = 16\n",
    "step_between_clips = 1\n",
    "frame_rate=10\n",
    "train=True\n",
    "transform = transform_vid\n",
    "output_format= 'TCHW'\n",
    "num_workers=40\n",
    "dataset = torchvision.datasets.UCF101(ucf_root, \n",
    "                                      annotation_path,\n",
    "                                      frames_per_clip,\n",
    "                                      step_between_clips=step_between_clips,\n",
    "                                      frame_rate=frame_rate,\n",
    "                                      fold=1,\n",
    "                                      train=train,\n",
    "                                      transform=transform,\n",
    "                                      output_format=output_format,\n",
    "                                      num_workers=num_workers)\n",
    "                                      \n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4dc0a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "video, _, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "56894972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 3, 224, 224])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b1d7bf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 43776])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7eb12c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67e84f",
   "metadata": {},
   "source": [
    "### Temp: compute the number of frames in g3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "142a3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_fpathlist(vid_root, subjdir, ds_rate=1):\n",
    "    \"\"\"\n",
    "    # read the image files inside vid_root/subj_dir into a list. \n",
    "    # makes sure they're all jpg. also sorts them so that the order of the frames is correct.\n",
    "    # subjdir = ['008MS']\n",
    "    \"\"\"\n",
    "    \n",
    "    fpathlist = sorted(list(Path(os.path.join(vid_root, subjdir)).iterdir()), \n",
    "                       key=lambda x: x.name)\n",
    "    fpathlist = [str(fpath) for fpath in fpathlist if fpath.suffix=='.jpg']\n",
    "    fpathlist = fpathlist[::ds_rate]\n",
    "    return fpathlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e62f6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_root='/N/project/infant_image_statistics/preproc_saber/JPG_30fps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24091f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3='BR+CW+EA+ED+JB+KI+LS+SB+TR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cd8cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g3.split('+')\n",
    "subj_dirs = g3\n",
    "ds_rate=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "01d69c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:25<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "gx_fpathlist = []\n",
    "for i_subj, subjdir in enumerate(tqdm(subj_dirs)):\n",
    "    gx_fpathlist += get_fpathlist(jpg_root, subjdir, ds_rate=ds_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a352f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918390\n"
     ]
    }
   ],
   "source": [
    "print(len(gx_fpathlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee3faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
